{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 03: Build FAISS Index & Test Retrieval\n",
        "\n",
        "## Goal\n",
        "\n",
        "Generate embeddings for all chunks, build a FAISS index, and test retrieval with example queries.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## FAISS Index Choice\n",
        "\n",
        "### IndexFlatIP vs IndexFlatL2\n",
        "\n",
        "- **IndexFlatIP (Inner Product)**: Requires normalized vectors; equivalent to cosine similarity\n",
        "- **IndexFlatL2 (Euclidean)**: No normalization needed; measures distance\n",
        "\n",
        "We'll use **IndexFlatIP** with normalized embeddings because:\n",
        "\n",
        "- Better semantic matching for text (cosine similarity)\n",
        "- Faster search for normalized vectors\n",
        "- Standard practice in semantic search\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Normalization & Search Latency\n",
        "\n",
        "- **Normalization**: Ensure all embeddings are L2-normalized before indexing\n",
        "- **Search latency**: IndexFlatIP is exact (no approximation), so search is O(n) but fast enough for our corpus size\n",
        "- **Future scaling**: For larger corpora, consider IndexIVFFlat or HNSW for approximate search\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Metadata Format for Citations\n",
        "\n",
        "Each chunk's metadata should include:\n",
        "\n",
        "- `book`: Source book name (e.g., \"iliad\", \"dorian\")\n",
        "- `para_idx_start`: First paragraph index in this chunk\n",
        "- `para_idx_end`: Last paragraph index in this chunk\n",
        "- `chunk_id`: Unique identifier for the chunk\n",
        "- `char_span`: Character start/end positions (optional, for precise citations)\n",
        "\n",
        "This metadata enables us to generate citations like:\n",
        "\n",
        "> \"[1] Quote text...\" â€” The Iliad, Book 1, paragraphs 5-7\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load Chunks from Previous Notebook\n",
        "\n",
        "Load the chunked data (or regenerate if needed).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Loaded 374 chunks from: ../data/interim/chunks/dorian_chunks.json\n",
            "   Book: dorian\n",
            "   Sample chunk ID: dorian_chunk_0\n",
            "   Total characters: 436,080\n"
          ]
        }
      ],
      "source": [
        "# Load chunks from saved file (created in notebook 02)\n",
        "import json\n",
        "import yaml\n",
        "from pathlib import Path\n",
        "\n",
        "def load_config(path=\"../configs/app.yaml\"):\n",
        "    with open(path, 'r') as f:\n",
        "        config = yaml.safe_load(f)\n",
        "    return config\n",
        "\n",
        "config = load_config()\n",
        "book_name = config['book']\n",
        "\n",
        "# Load chunks from JSON file\n",
        "chunks_file = Path(f\"../data/interim/chunks/{book_name}_chunks.json\")\n",
        "\n",
        "if not chunks_file.exists():\n",
        "    raise FileNotFoundError(\n",
        "        f\"Chunks file not found: {chunks_file}\\n\"\n",
        "        f\"Please run notebook 02 first to generate chunks.\"\n",
        "    )\n",
        "\n",
        "with open(chunks_file, 'r', encoding='utf-8') as f:\n",
        "    chunks = json.load(f)\n",
        "\n",
        "print(f\"âœ… Loaded {len(chunks)} chunks from: {chunks_file}\")\n",
        "print(f\"   Book: {book_name}\")\n",
        "print(f\"   Sample chunk ID: {chunks[0]['id'] if chunks else 'N/A'}\")\n",
        "\n",
        "# Extract chunk texts and metadata for embedding\n",
        "chunk_texts = [chunk['text'] for chunk in chunks]\n",
        "print(f\"   Total characters: {sum(len(text) for text in chunk_texts):,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Memory Check\n",
        "\n",
        "Before embedding, let's verify we have enough chunks and check memory usage.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“Š Chunk Statistics:\n",
            "   Number of chunks: 374\n",
            "   Total characters: 436,080\n",
            "   Average chunk size: 1166 chars\n",
            "\n",
            "ðŸ’¾ Estimated memory for embeddings: 0.55 MB\n",
            "   (This should be manageable for most systems)\n",
            "\n",
            "âœ… Ready to proceed with embedding\n"
          ]
        }
      ],
      "source": [
        "# Quick memory check\n",
        "import sys\n",
        "\n",
        "print(f\"ðŸ“Š Chunk Statistics:\")\n",
        "print(f\"   Number of chunks: {len(chunks)}\")\n",
        "print(f\"   Total characters: {sum(len(chunk['text']) for chunk in chunks):,}\")\n",
        "print(f\"   Average chunk size: {sum(len(chunk['text']) for chunk in chunks) / len(chunks):.0f} chars\")\n",
        "\n",
        "# Estimate memory needed for embeddings (384 dims * 4 bytes * num_chunks)\n",
        "estimated_mb = (384 * 4 * len(chunks)) / (1024 * 1024)\n",
        "print(f\"\\nðŸ’¾ Estimated memory for embeddings: {estimated_mb:.2f} MB\")\n",
        "print(f\"   (This should be manageable for most systems)\")\n",
        "\n",
        "# Check if we can proceed\n",
        "if len(chunks) == 0:\n",
        "    raise ValueError(\"No chunks loaded! Please run notebook 02 first.\")\n",
        "    \n",
        "print(f\"\\nâœ… Ready to proceed with embedding\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test with Small Sample First (Optional)\n",
        "\n",
        "If you're experiencing crashes, test with a small sample first to isolate the issue.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ§ª TEST MODE: Using first 10 chunks only\n",
            "   Testing with 10 chunks\n"
          ]
        }
      ],
      "source": [
        "# OPTIONAL: Test with first 10 chunks to verify everything works\n",
        "# Uncomment below to test with a small sample first\n",
        "\n",
        "TEST_MODE = True\n",
        "if TEST_MODE:\n",
        "    print(\"ðŸ§ª TEST MODE: Using first 10 chunks only\")\n",
        "    chunks = chunks[:10]\n",
        "    print(f\"   Testing with {len(chunks)} chunks\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Build Full Embeddings & FAISS Index\n",
        "\n",
        "Embed all chunks and build the FAISS index.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ“š Preparing to embed 10 chunks...\n",
            "   Total characters: 11,372\n",
            "ðŸ“š Embedding using sentence-transformers/all-MiniLM-L6-v2...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fff9ade648a941499b362890bc385e4d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Embedded 10 chunks\n",
            "   Embedding shape: (10, 384)\n",
            "   Memory usage: 0.01 MB\n",
            "ðŸ”¨ Building FAISS index...\n",
            "âœ… Built FAISS index with 10 vectors\n",
            "âœ… Saved index to: ../data/index/index.faiss\n",
            "âœ… Saved metadata to: ../data/index/metadata.parquet\n",
            "   Index size: 10 vectors\n",
            "   Metadata rows: 10\n",
            "âœ… Saved index to ../data/index\n",
            "\n",
            "ðŸŽ‰ Successfully built and saved FAISS index!\n"
          ]
        }
      ],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Build full embeddings and FAISS index; persist to data/index/.\n",
        "\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import importlib\n",
        "import gc\n",
        "\n",
        "sys.path.append(str(Path(\"..\").resolve()))\n",
        "from src import embed_index\n",
        "importlib.reload(embed_index)  # Reload to get latest changes\n",
        "from src.embed_index import embed_texts, build_faiss_index, save_index\n",
        "\n",
        "# 1. Embed all chunk texts\n",
        "# 2. Build FAISS index (IndexFlatIP with normalized vectors)\n",
        "# 3. Save index and metadata to data/index/\n",
        "\n",
        "# Use the chunks already loaded (they're already filtered by book from notebook 02)\n",
        "chunk_texts = [chunk['text'] for chunk in chunks]\n",
        "\n",
        "# Prepare metadata rows for saving\n",
        "meta_rows = []\n",
        "for chunk in chunks:\n",
        "    meta_rows.append({\n",
        "        'chunk_id': chunk['id'],\n",
        "        'book': chunk['meta']['book'],\n",
        "        'para_idx_start': chunk['meta']['para_idx_start'],\n",
        "        'para_idx_end': chunk['meta']['para_idx_end'],\n",
        "        'char_count': chunk['meta']['char_count']\n",
        "    })\n",
        "\n",
        "print(f\"ðŸ“š Preparing to embed {len(chunk_texts)} chunks...\")\n",
        "print(f\"   Total characters: {sum(len(text) for text in chunk_texts):,}\")\n",
        "\n",
        "# Embed in batches to avoid memory issues\n",
        "try:\n",
        "    print(f\"ðŸ“š Embedding using {config['embedding_model']}...\")\n",
        "    embeddings, model = embed_texts(chunk_texts, config['embedding_model'])\n",
        "    print(f\"âœ… Embedded {len(chunk_texts)} chunks\")\n",
        "    print(f\"   Embedding shape: {embeddings.shape}\")\n",
        "    print(f\"   Memory usage: {embeddings.nbytes / 1024 / 1024:.2f} MB\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error during embedding: {e}\")\n",
        "    raise\n",
        "\n",
        "# Free up memory by deleting the model if not needed\n",
        "del model\n",
        "gc.collect()\n",
        "\n",
        "# Build FAISS index\n",
        "try:\n",
        "    print(f\"ðŸ”¨ Building FAISS index...\")\n",
        "    index = build_faiss_index(embeddings)\n",
        "    print(f\"âœ… Built FAISS index with {index.ntotal} vectors\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error building index: {e}\")\n",
        "    raise\n",
        "\n",
        "# Save index and metadata\n",
        "out_dir = \"../data/index\"\n",
        "try:\n",
        "    save_index(index, meta_rows, out_dir)\n",
        "    print(f\"âœ… Saved index to {out_dir}\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error saving index: {e}\")\n",
        "    raise\n",
        "\n",
        "print(f\"\\nðŸŽ‰ Successfully built and saved FAISS index!\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Load Index & Test Retrieval\n",
        "\n",
        "Load the saved index and test retrieval with example queries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Loaded index: 10 vectors, dimension 384\n",
            "âœ… Loaded metadata: 10 rows\n",
            "Index loaded successfully\n",
            "Index info:\n",
            "  Number of vectors: 10\n",
            "  Dimension: 384\n",
            "  Index type: <class 'faiss.swigfaiss.IndexFlat'>\n",
            "  Metadata shape: (10, 5)\n",
            "  Metadata columns: Index(['chunk_id', 'book', 'para_idx_start', 'para_idx_end', 'char_count'], dtype='object')\n",
            "\n",
            "Model loaded successfully. Config:\n",
            "  Embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
            "  Book: dorian\n",
            "  Chunks file loaded successfully: ../data/interim/chunks/dorian_chunks.json\n",
            "Ready to test retrieval\n",
            ".   Top-k: 5\n"
          ]
        }
      ],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Load index & metadata; test a few queries.\n",
        "\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "\n",
        "from src.embed_index import load_index\n",
        "from src.retrieve import retrieve\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import faiss\n",
        "\n",
        "# 1. Load index and metadata\n",
        "# 2. Load embedding model\n",
        "# 3. Test retrieval with example queries\n",
        "in_dir = \"../data/index\"\n",
        "index, metadata_df = load_index(in_dir)\n",
        "print(\"Index loaded successfully\")\n",
        "print(\"Index info:\")\n",
        "print(f\"  Number of vectors: {index.ntotal}\")\n",
        "print(f\"  Dimension: {index.d}\")\n",
        "print(f\"  Index type: {type(index)}\")\n",
        "print(f\"  Metadata shape: {metadata_df.shape}\")\n",
        "print(f\"  Metadata columns: {metadata_df.columns}\")\n",
        "\n",
        "config = load_config()\n",
        "model = SentenceTransformer(config['embedding_model'])\n",
        "print(\"\\nModel loaded successfully. Config:\")\n",
        "print(f\"  Embedding model: {config['embedding_model']}\")\n",
        "print(f\"  Book: {config['book']}\")\n",
        "\n",
        "chunks_lookup = None\n",
        "try:\n",
        "    book_name = config['book']\n",
        "    chunks_file = Path(f\"../data/interim/chunks/{book_name}_chunks.json\")\n",
        "    if chunks_file.exists():\n",
        "        with open(chunks_file, 'r', encoding='utf-8') as f:\n",
        "            chunks_lookup = json.load(f)\n",
        "        print(f\"  Chunks file loaded successfully: {chunks_file}\")\n",
        "    else:\n",
        "        print(f\"âŒ Chunks file not found: {chunks_file}\")\n",
        "except Exception as e:\n",
        "    print(f\"âŒ Error loading chunks: {e}\")\n",
        "    raise\n",
        "\n",
        "def embed_query_fn(query: str) -> np.ndarray:\n",
        "    embedding = model.encode([query],normalize_embeddings=True, show_progress_bar=True)\n",
        "    embedding = np.array(embedding, dtype=np.float32)\n",
        "    faiss.normalize_L2(embedding)\n",
        "    return embedding[0]\n",
        "\n",
        "print(\"Ready to test retrieval\")\n",
        "print(f\".   Top-k: {config.get('top_k', 5)}\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âš ï¸  chunks_lookup is a list (length: 374), converting to dict...\n",
            "âœ… Converted to dictionary with 374 entries\n",
            "   Sample keys: ['dorian_chunk_0', 'dorian_chunk_1', 'dorian_chunk_2']\n"
          ]
        }
      ],
      "source": [
        "# Fix chunks_lookup: Convert from list to dictionary\n",
        "# The JSON file is a list, but retrieve() needs a dict mapping chunk_id -> chunk\n",
        "\n",
        "if chunks_lookup is not None:\n",
        "    # Check if it's a list (wrong) or dict (correct)\n",
        "    if isinstance(chunks_lookup, list):\n",
        "        print(f\"âš ï¸  chunks_lookup is a list (length: {len(chunks_lookup)}), converting to dict...\")\n",
        "        # Convert list to dictionary: chunk['id'] -> chunk\n",
        "        chunks_lookup = {chunk['id']: chunk for chunk in chunks_lookup}\n",
        "        print(f\"âœ… Converted to dictionary with {len(chunks_lookup)} entries\")\n",
        "        print(f\"   Sample keys: {list(chunks_lookup.keys())[:3]}\")\n",
        "    elif isinstance(chunks_lookup, dict):\n",
        "        print(f\"âœ… chunks_lookup is already a dictionary with {len(chunks_lookup)} entries\")\n",
        "    else:\n",
        "        print(f\"âŒ chunks_lookup is unexpected type: {type(chunks_lookup)}\")\n",
        "else:\n",
        "    print(\"âŒ chunks_lookup is None - need to reload chunks\")\n",
        "    # Reload and convert properly\n",
        "    book_name = config['book']\n",
        "    chunks_file = Path(f\"../data/interim/chunks/{book_name}_chunks.json\")\n",
        "    if chunks_file.exists():\n",
        "        with open(chunks_file, 'r', encoding='utf-8') as f:\n",
        "            chunks_list = json.load(f)\n",
        "            chunks_lookup = {chunk['id']: chunk for chunk in chunks_list}\n",
        "        print(f\"âœ… Reloaded and converted {len(chunks_lookup)} chunks to dictionary\")\n",
        "    else:\n",
        "        print(f\"âŒ Chunks file not found: {chunks_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Better filter function to exclude TOC/header chunks\n",
        "import re\n",
        "\n",
        "def is_toc_or_header_chunk(result):\n",
        "    \"\"\"\n",
        "    Detect if a chunk is a TOC, header, or low-content chunk.\n",
        "    Returns True if it should be filtered out.\n",
        "    \"\"\"\n",
        "    text = result['text']\n",
        "    chunk_id = result.get('chunk_id', '')\n",
        "    meta = result.get('meta', {})\n",
        "    \n",
        "    # Filter out chunk 0 (usually TOC/preface)\n",
        "    if chunk_id.endswith('_chunk_0') or meta.get('para_idx_start', -1) == 0:\n",
        "        # But allow it if it has substantial content (not just TOC)\n",
        "        if 'Contents' in text and text.count('CHAPTER') > 5:\n",
        "            return True  # It's a TOC\n",
        "    \n",
        "    # Filter very short chunks\n",
        "    if len(text) < 150:\n",
        "        return True\n",
        "    \n",
        "    # Filter chunks with too many newlines (indicates headers/TOC)\n",
        "    newline_ratio = text.count('\\n') / len(text) if len(text) > 0 else 0\n",
        "    if newline_ratio > 0.15:  # More than 15% newlines\n",
        "        return True\n",
        "    \n",
        "    # Filter chunks that are mostly chapter titles\n",
        "    lines = text.split('\\n')\n",
        "    chapter_lines = [line for line in lines if 'CHAPTER' in line.upper() or re.match(r'^CHAPTER\\s+[IVX]+', line, re.IGNORECASE)]\n",
        "    if len(chapter_lines) > 3:  # More than 3 chapter title lines\n",
        "        return True\n",
        "    \n",
        "    # Filter chunks that start with title/author/contents pattern\n",
        "    first_100 = text[:100].lower()\n",
        "    if ('contents' in first_100 and 'chapter' in first_100) or \\\n",
        "       (text.startswith('The Picture of') and 'by Oscar Wilde' in first_100):\n",
        "        # Check if it's mostly TOC (many short lines)\n",
        "        short_lines = [line for line in lines[:30] if len(line.strip()) < 50]\n",
        "        if len(short_lines) > 10:  # More than 10 short lines in first 30\n",
        "            return True\n",
        "    \n",
        "    return False\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Example Queries & Manual Relevance Check\n",
        "\n",
        "Test with queries like:\n",
        "\n",
        "- \"How does Homer portray Achilles' anger in Book 1?\"\n",
        "- \"What does Lord Henry claim about influence on the young?\"\n",
        "- \"Where does the poem describe the shield of Achilles?\"\n",
        "\n",
        "For each query, manually judge whether the retrieved snippets are relevant. This helps validate:\n",
        "\n",
        "1. Embedding quality (semantic similarity)\n",
        "2. Chunk size appropriateness (not too fragmented, not too broad)\n",
        "3. Retrieval ranking (most relevant chunks appear first)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6bbe076d892642b9964859ceb4ad3f36",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing filter on current results...\n",
            "\n",
            "Result 1 (dorian_chunk_1): âœ… KEEP\n",
            "\n",
            "Result 2 (dorian_chunk_4): âœ… KEEP\n",
            "\n",
            "Result 3 (dorian_chunk_5): âœ… KEEP\n",
            "\n",
            "Result 4 (dorian_chunk_8): âœ… KEEP\n",
            "\n",
            "Result 5 (dorian_chunk_6): âœ… KEEP\n",
            "\n",
            "\n",
            "ðŸ“Š Filtering Results:\n",
            "   Original: 5 chunks\n",
            "   Filtered: 5 chunks\n",
            "   Removed: 0 chunks\n",
            "\n",
            "Result 1:\n",
            "  Score: 0.4259\n",
            "  Chunk ID: dorian_chunk_1\n",
            "  Book: dorian\n",
            "  Paragraph range: 8-10\n",
            "  Text: There is no such thing as a moral or an immoral book. Books are well\n",
            "written, or badly written. That is all.\n",
            "\n",
            "The nineteenth century dislike of realism is the rage of Caliban seeing\n",
            "his own face in a glass.\n",
            "\n",
            "The nineteenth century dislike of romanticism is the rage of Caliban\n",
            "not seeing his own face...\n",
            "\n",
            "Result 2:\n",
            "  Score: 0.4170\n",
            "  Chunk ID: dorian_chunk_4\n",
            "  Book: dorian\n",
            "  Paragraph range: 19-20\n",
            "  Text: â€œI donâ€™t think I shall send it anywhere,â€ he answered, tossing his head\n",
            "back in that odd way that used to make his friends laugh at him at\n",
            "Oxford. â€œNo, I wonâ€™t send it anywhere.â€\n",
            "\n",
            "Lord Henry elevated his eyebrows and looked at him in amazement through\n",
            "the thin blue wreaths of smoke that curled up in...\n",
            "\n",
            "Result 3:\n",
            "  Score: 0.3742\n",
            "  Chunk ID: dorian_chunk_5\n",
            "  Book: dorian\n",
            "  Paragraph range: 21-24\n",
            "  Text: â€œI know you will laugh at me,â€ he replied, â€œbut I really canâ€™t exhibit\n",
            "it. I have put too much of myself into it.â€\n",
            "\n",
            "Lord Henry stretched himself out on the divan and laughed.\n",
            "\n",
            "â€œYes, I knew you would; but it is quite true, all the same.â€\n",
            "\n",
            "â€œToo much of yourself in it! Upon my word, Basil, I didnâ€™t kno...\n",
            "\n",
            "Result 4:\n",
            "  Score: 0.3656\n",
            "  Chunk ID: dorian_chunk_8\n",
            "  Book: dorian\n",
            "  Paragraph range: 31-33\n",
            "  Text: â€œI hate the way you talk about your married life, Harry,â€ said Basil\n",
            "Hallward, strolling towards the door that led into the garden. â€œI\n",
            "believe that you are really a very good husband, but that you are\n",
            "thoroughly ashamed of your own virtues. You are an extraordinary\n",
            "fellow. You never say a moral thin...\n",
            "\n",
            "Result 5:\n",
            "  Score: 0.3285\n",
            "  Chunk ID: dorian_chunk_6\n",
            "  Book: dorian\n",
            "  Paragraph range: 25-25\n",
            "  Text: â€œYou donâ€™t understand me, Harry,â€ answered the artist. â€œOf course I am\n",
            "not like him. I know that perfectly well. Indeed, I should be sorry to\n",
            "look like him. You shrug your shoulders? I am telling you the truth.\n",
            "There is a fatality about all physical and intellectual distinction,\n",
            "the sort of fatality...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f95d0d6295f34798aa27ed71ffa248d8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Result 1:\n",
            "  Score: 0.5341\n",
            "  Chunk ID: dorian_chunk_9\n",
            "  Book: dorian\n",
            "  Paragraph range: 34-40\n",
            "  Text: â€œWhat is that?â€ said the painter, keeping his eyes fixed on the ground.\n",
            "\n",
            "â€œYou know quite well.â€\n",
            "\n",
            "â€œI do not, Harry.â€\n",
            "\n",
            "â€œWell, I will tell you what it is. I want you to explain to me why you\n",
            "wonâ€™t exhibit Dorian Grayâ€™s picture. I want the real reason.â€\n",
            "\n",
            "â€œI told you the real reason.â€\n",
            "\n",
            "â€œNo, you did not. ...\n",
            "\n",
            "Result 2:\n",
            "  Score: 0.5257\n",
            "  Chunk ID: dorian_chunk_3\n",
            "  Book: dorian\n",
            "  Paragraph range: 16-18\n",
            "  Text: In the centre of the room, clamped to an upright easel, stood the\n",
            "full-length portrait of a young man of extraordinary personal beauty,\n",
            "and in front of it, some little distance away, was sitting the artist\n",
            "himself, Basil Hallward, whose sudden disappearance some years ago\n",
            "caused, at the time, such p...\n",
            "\n",
            "Result 3:\n",
            "  Score: 0.4413\n",
            "  Chunk ID: dorian_chunk_2\n",
            "  Book: dorian\n",
            "  Paragraph range: 11-15\n",
            "  Text: All art is quite useless.\n",
            "\n",
            "OSCAR WILDE\n",
            "\n",
            "CHAPTER I.\n",
            "\n",
            "The studio was filled with the rich odour of roses, and when the light\n",
            "summer wind stirred amidst the trees of the garden, there came through\n",
            "the open door the heavy scent of the lilac, or the more delicate\n",
            "perfume of the pink-flowering thorn.\n",
            "\n",
            "Fro...\n",
            "\n",
            "Result 4:\n",
            "  Score: 0.4329\n",
            "  Chunk ID: dorian_chunk_0\n",
            "  Book: dorian\n",
            "  Paragraph range: 0-7\n",
            "  Text: The Picture of Dorian Gray\n",
            "\n",
            "by Oscar Wilde\n",
            "\n",
            "Contents\n",
            "\n",
            "THE PREFACE\n",
            "CHAPTER I.\n",
            "CHAPTER II.\n",
            "CHAPTER III.\n",
            "CHAPTER IV.\n",
            "CHAPTER V.\n",
            "CHAPTER VI.\n",
            "CHAPTER VII.\n",
            "CHAPTER VIII.\n",
            "CHAPTER IX.\n",
            "CHAPTER X.\n",
            "CHAPTER XI.\n",
            "CHAPTER XII.\n",
            "CHAPTER XIII.\n",
            "CHAPTER XIV.\n",
            "CHAPTER XV.\n",
            "CHAPTER XVI.\n",
            "CHAPTER XVII.\n",
            "CHAPTER XVIII.\n",
            "CHAPTER...\n",
            "\n",
            "Result 5:\n",
            "  Score: 0.3436\n",
            "  Chunk ID: dorian_chunk_4\n",
            "  Book: dorian\n",
            "  Paragraph range: 19-20\n",
            "  Text: â€œI donâ€™t think I shall send it anywhere,â€ he answered, tossing his head\n",
            "back in that odd way that used to make his friends laugh at him at\n",
            "Oxford. â€œNo, I wonâ€™t send it anywhere.â€\n",
            "\n",
            "Lord Henry elevated his eyebrows and looked at him in amazement through\n",
            "the thin blue wreaths of smoke that curled up in...\n"
          ]
        }
      ],
      "source": [
        "# Test queries and display top-k results\n",
        "# For each query, show:\n",
        "# - Query text\n",
        "# - Top 3-5 retrieved chunks with scores\n",
        "# - Manual relevance judgment (relevant/partially relevant/not relevant)\n",
        "\n",
        "query = \"Lord Henry says all influence is immoral\"\n",
        "\n",
        "results = retrieve(\n",
        "    query=query, \n",
        "    index=index, \n",
        "    embed_fn=embed_query_fn, \n",
        "    metadata_df=metadata_df, \n",
        "    chunks_lookup=chunks_lookup,\n",
        "    k=config.get('top_k', 5)\n",
        ")\n",
        "\n",
        "# After retrieval, filter out low-content chunks\n",
        "print(\"Testing filter on current results...\\n\")\n",
        "\n",
        "for i, result in enumerate(results, 1):\n",
        "    should_filter = is_toc_or_header_chunk(result)\n",
        "    status = \"âŒ FILTER OUT\" if should_filter else \"âœ… KEEP\"\n",
        "    print(f\"Result {i} ({result['chunk_id']}): {status}\")\n",
        "    if should_filter:\n",
        "        print(f\"  Reason: TOC/header detected\")\n",
        "    print()\n",
        "\n",
        "# Apply the filter\n",
        "filtered_results = [r for r in results if not is_toc_or_header_chunk(r)]\n",
        "\n",
        "print(f\"\\nðŸ“Š Filtering Results:\")\n",
        "print(f\"   Original: {len(results)} chunks\")\n",
        "print(f\"   Filtered: {len(filtered_results)} chunks\")\n",
        "print(f\"   Removed: {len(results) - len(filtered_results)} chunks\")\n",
        "\n",
        "for i, result in enumerate(filtered_results, 1):\n",
        "    print(f\"\\nResult {i}:\")\n",
        "    print(f\"  Score: {result['score']:.4f}\")\n",
        "    print(f\"  Chunk ID: {result['chunk_id']}\")\n",
        "    print(f\"  Book: {result['meta']['book']}\")\n",
        "    print(f\"  Paragraph range: {result['meta']['para_idx_start']}-{result['meta']['para_idx_end']}\")\n",
        "    print(f\"  Text: {result['text'][:300]}...\")\n",
        "\n",
        "query_2 = \"Describe the appearance of the portrait painting of the young man\"\n",
        "\n",
        "results_2 = retrieve(\n",
        "    query=query_2, \n",
        "    index=index, \n",
        "    embed_fn=embed_query_fn, \n",
        "    metadata_df=metadata_df, \n",
        "    chunks_lookup=chunks_lookup,\n",
        "    k=config.get('top_k', 5)\n",
        ")\n",
        "\n",
        "# After retrieval, filter out low-content chunks\n",
        "filtered_results_2 = []\n",
        "\n",
        "for result in results_2:\n",
        "    text = result['text']\n",
        "    # Skip if it's mostly headers/TOC (lots of all caps, short lines, etc.)\n",
        "    if len(text) < 200 or text.count('\\n') / len(text) > 0.1:\n",
        "        continue\n",
        "    filtered_results_2.append(result)\n",
        "\n",
        "for i, result in enumerate(filtered_results_2, 1):\n",
        "    print(f\"\\nResult {i}:\")\n",
        "    print(f\"  Score: {result['score']:.4f}\")\n",
        "    print(f\"  Chunk ID: {result['chunk_id']}\")\n",
        "    print(f\"  Book: {result['meta']['book']}\")\n",
        "    print(f\"  Paragraph range: {result['meta']['para_idx_start']}-{result['meta']['para_idx_end']}\")\n",
        "    print(f\"  Text: {result['text'][:300]}...\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "At this point, you should have:\n",
        "\n",
        "- âœ… Full FAISS index built and saved to `data/index/`\n",
        "- âœ… Metadata persisted alongside the index\n",
        "- âœ… Retrieval tested with example queries\n",
        "- âœ… Manual validation that retrieved chunks are relevant\n",
        "\n",
        "**Next notebook**: Build a small QA evaluation set, test answer composition, and wire up the Gradio demo.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Codecademy ML",
      "language": "python",
      "name": "codeacademy"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
