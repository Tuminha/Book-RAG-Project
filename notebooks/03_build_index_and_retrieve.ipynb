{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 03: Build FAISS Index & Test Retrieval\n",
    "\n",
    "## Goal\n",
    "\n",
    "Generate embeddings for all chunks, build a FAISS index, and test retrieval with example queries.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FAISS Index Choice\n",
    "\n",
    "### IndexFlatIP vs IndexFlatL2\n",
    "\n",
    "- **IndexFlatIP (Inner Product)**: Requires normalized vectors; equivalent to cosine similarity\n",
    "- **IndexFlatL2 (Euclidean)**: No normalization needed; measures distance\n",
    "\n",
    "We'll use **IndexFlatIP** with normalized embeddings because:\n",
    "\n",
    "- Better semantic matching for text (cosine similarity)\n",
    "- Faster search for normalized vectors\n",
    "- Standard practice in semantic search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization & Search Latency\n",
    "\n",
    "- **Normalization**: Ensure all embeddings are L2-normalized before indexing\n",
    "- **Search latency**: IndexFlatIP is exact (no approximation), so search is O(n) but fast enough for our corpus size\n",
    "- **Future scaling**: For larger corpora, consider IndexIVFFlat or HNSW for approximate search\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metadata Format for Citations\n",
    "\n",
    "Each chunk's metadata should include:\n",
    "\n",
    "- `book`: Source book name (e.g., \"iliad\", \"dorian\")\n",
    "- `para_idx_start`: First paragraph index in this chunk\n",
    "- `para_idx_end`: Last paragraph index in this chunk\n",
    "- `chunk_id`: Unique identifier for the chunk\n",
    "- `char_span`: Character start/end positions (optional, for precise citations)\n",
    "\n",
    "This metadata enables us to generate citations like:\n",
    "\n",
    "> \"[1] Quote text...\" ‚Äî The Iliad, Book 1, paragraphs 5-7\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Load Chunks from Previous Notebook\n",
    "\n",
    "Load the chunked data (or regenerate if needed).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 374 chunks from: ../data/interim/chunks/dorian_chunks.json\n",
      "   Book: dorian\n",
      "   Sample chunk ID: dorian_chunk_0\n",
      "   Total characters: 436,080\n"
     ]
    }
   ],
   "source": [
    "# Load chunks from saved file (created in notebook 02)\n",
    "import json\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "def load_config(path=\"../configs/app.yaml\"):\n",
    "    with open(path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    return config\n",
    "\n",
    "config = load_config()\n",
    "book_name = config['book']\n",
    "\n",
    "# Load chunks from JSON file\n",
    "chunks_file = Path(f\"../data/interim/chunks/{book_name}_chunks.json\")\n",
    "\n",
    "if not chunks_file.exists():\n",
    "    raise FileNotFoundError(\n",
    "        f\"Chunks file not found: {chunks_file}\\n\"\n",
    "        f\"Please run notebook 02 first to generate chunks.\"\n",
    "    )\n",
    "\n",
    "with open(chunks_file, 'r', encoding='utf-8') as f:\n",
    "    chunks = json.load(f)\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(chunks)} chunks from: {chunks_file}\")\n",
    "print(f\"   Book: {book_name}\")\n",
    "print(f\"   Sample chunk ID: {chunks[0]['id'] if chunks else 'N/A'}\")\n",
    "\n",
    "# Extract chunk texts and metadata for embedding\n",
    "chunk_texts = [chunk['text'] for chunk in chunks]\n",
    "print(f\"   Total characters: {sum(len(text) for text in chunk_texts):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Check\n",
    "\n",
    "Before embedding, let's verify we have enough chunks and check memory usage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Chunk Statistics:\n",
      "   Number of chunks: 374\n",
      "   Total characters: 436,080\n",
      "   Average chunk size: 1166 chars\n",
      "\n",
      "üíæ Estimated memory for embeddings: 0.55 MB\n",
      "   (This should be manageable for most systems)\n",
      "\n",
      "‚úÖ Ready to proceed with embedding\n"
     ]
    }
   ],
   "source": [
    "# Quick memory check\n",
    "import sys\n",
    "\n",
    "print(f\"üìä Chunk Statistics:\")\n",
    "print(f\"   Number of chunks: {len(chunks)}\")\n",
    "print(f\"   Total characters: {sum(len(chunk['text']) for chunk in chunks):,}\")\n",
    "print(f\"   Average chunk size: {sum(len(chunk['text']) for chunk in chunks) / len(chunks):.0f} chars\")\n",
    "\n",
    "# Estimate memory needed for embeddings (384 dims * 4 bytes * num_chunks)\n",
    "estimated_mb = (384 * 4 * len(chunks)) / (1024 * 1024)\n",
    "print(f\"\\nüíæ Estimated memory for embeddings: {estimated_mb:.2f} MB\")\n",
    "print(f\"   (This should be manageable for most systems)\")\n",
    "\n",
    "# Check if we can proceed\n",
    "if len(chunks) == 0:\n",
    "    raise ValueError(\"No chunks loaded! Please run notebook 02 first.\")\n",
    "    \n",
    "print(f\"\\n‚úÖ Ready to proceed with embedding\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with Small Sample First (Optional)\n",
    "\n",
    "If you're experiencing crashes, test with a small sample first to isolate the issue.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Using all 374 chunks\n"
     ]
    }
   ],
   "source": [
    "# OPTIONAL: Test with first 10 chunks to verify everything works\n",
    "# Uncomment below to test with a small sample first\n",
    "\n",
    "TEST_MODE = False  # Toggle to True only for quick smoke-tests\n",
    "TEST_CHUNK_LIMIT = 10\n",
    "\n",
    "ORIGINAL_CHUNK_COUNT = len(chunks)\n",
    "\n",
    "if TEST_MODE:\n",
    "    limit = min(TEST_CHUNK_LIMIT, ORIGINAL_CHUNK_COUNT)\n",
    "    print(f\"üß™ TEST MODE: Using first {limit} of {ORIGINAL_CHUNK_COUNT} chunks\")\n",
    "    chunks = chunks[:limit]\n",
    "    print(\"   ‚ö†Ô∏è Index persistence is disabled while TEST_MODE is True.\")\n",
    "else:\n",
    "    print(f\"üöÄ Using all {ORIGINAL_CHUNK_COUNT} chunks\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Build Full Embeddings & FAISS Index\n",
    "\n",
    "Embed all chunks and build the FAISS index.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Preparing to embed 374 chunks...\n",
      "   Total characters: 436,080\n",
      "üìö Embedding using sentence-transformers/all-MiniLM-L6-v2...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f2690e9b9784542a082510f7e99fbd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/12 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Embedded 374 chunks\n",
      "   Embedding shape: (374, 384)\n",
      "   Memory usage: 0.55 MB\n",
      "üî® Building FAISS index...\n",
      "‚úÖ Built FAISS index with 374 vectors\n",
      "‚úÖ Saved index to: ../data/index/index.faiss\n",
      "‚úÖ Saved metadata to: ../data/index/metadata.parquet\n",
      "   Index size: 374 vectors\n",
      "   Metadata rows: 374\n",
      "‚úÖ Saved index to ../data/index\n",
      "\n",
      "üéâ Successfully built and saved FAISS index!\n"
     ]
    }
   ],
   "source": [
    "# === TODO (you code this) ===\n",
    "# Build full embeddings and FAISS index; persist to data/index/.\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import importlib\n",
    "import gc\n",
    "\n",
    "sys.path.append(str(Path('..').resolve()))\n",
    "from src import embed_index\n",
    "importlib.reload(embed_index)  # Reload to get latest changes\n",
    "from src.embed_index import embed_texts, build_faiss_index, save_index\n",
    "\n",
    "# Track run configuration\n",
    "TEST_MODE = globals().get('TEST_MODE', False)\n",
    "ORIGINAL_CHUNK_COUNT = globals().get('ORIGINAL_CHUNK_COUNT', len(chunks))\n",
    "\n",
    "# 1. Embed all chunk texts\n",
    "# 2. Build FAISS index (IndexFlatIP with normalized vectors)\n",
    "# 3. Save index and metadata to data/index/\n",
    "\n",
    "# Use the chunks already loaded (they're already filtered by book from notebook 02)\n",
    "chunk_texts = [chunk['text'] for chunk in chunks]\n",
    "current_chunk_count = len(chunk_texts)\n",
    "subset_warning = current_chunk_count < ORIGINAL_CHUNK_COUNT\n",
    "\n",
    "if subset_warning:\n",
    "    print(f\"‚ö†Ô∏è Working with {current_chunk_count} of {ORIGINAL_CHUNK_COUNT} chunks\")\n",
    "    print(\"   Set TEST_MODE = False to embed the full corpus before persisting the index\")\n",
    "\n",
    "# Prepare metadata rows for saving\n",
    "meta_rows = []\n",
    "for chunk in chunks:\n",
    "    meta_rows.append({\n",
    "        'chunk_id': chunk['id'],\n",
    "        'book': chunk['meta']['book'],\n",
    "        'para_idx_start': chunk['meta']['para_idx_start'],\n",
    "        'para_idx_end': chunk['meta']['para_idx_end'],\n",
    "        'char_count': chunk['meta']['char_count']\n",
    "    })\n",
    "\n",
    "print(f\"üìö Preparing to embed {current_chunk_count} chunks...\")\n",
    "print(f\"   Total characters: {sum(len(text) for text in chunk_texts):,}\")\n",
    "\n",
    "# Embed in batches to avoid memory issues\n",
    "try:\n",
    "    print(f\"üìö Embedding using {config['embedding_model']}...\")\n",
    "    embeddings, model = embed_texts(chunk_texts, config['embedding_model'])\n",
    "    print(f\"‚úÖ Embedded {current_chunk_count} chunks\")\n",
    "    print(f\"   Embedding shape: {embeddings.shape}\")\n",
    "    print(f\"   Memory usage: {embeddings.nbytes / 1024 / 1024:.2f} MB\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during embedding: {e}\")\n",
    "    raise\n",
    "\n",
    "# Free up memory by deleting the model if not needed\n",
    "del model\n",
    "gc.collect()\n",
    "\n",
    "# Build FAISS index\n",
    "try:\n",
    "    print(f\"üî® Building FAISS index...\")\n",
    "    index = build_faiss_index(embeddings)\n",
    "    print(f\"‚úÖ Built FAISS index with {index.ntotal} vectors\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error building index: {e}\")\n",
    "    raise\n",
    "\n",
    "# Save index and metadata\n",
    "out_dir = '../data/index'\n",
    "skip_persist = TEST_MODE and subset_warning\n",
    "\n",
    "if skip_persist:\n",
    "    print(\"‚ö†Ô∏è TEST MODE active: skipping save_index to avoid overwriting the full artifacts.\")\n",
    "    print(\"   Toggle TEST_MODE = False and rerun this cell when you're ready to persist the full index.\")\n",
    "else:\n",
    "    try:\n",
    "        save_index(index, meta_rows, out_dir)\n",
    "        print(f\"‚úÖ Saved index to {out_dir}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving index: {e}\")\n",
    "        raise\n",
    "\n",
    "if skip_persist:\n",
    "    print(\"\\n‚ÑπÔ∏è Index + metadata objects are available in-memory for experimentation, but disk files were left untouched.\")\n",
    "else:\n",
    "    print(\"\\nüéâ Successfully built and saved FAISS index!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Index & Test Retrieval\n",
    "\n",
    "Load the saved index and test retrieval with example queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded index: 374 vectors, dimension 384\n",
      "‚úÖ Loaded metadata: 374 rows\n",
      "Index loaded successfully\n",
      "Index info:\n",
      "  Number of vectors: 374\n",
      "  Dimension: 384\n",
      "  Index type: <class 'faiss.swigfaiss.IndexFlat'>\n",
      "  Metadata shape: (374, 5)\n",
      "  Metadata columns: Index(['chunk_id', 'book', 'para_idx_start', 'para_idx_end', 'char_count'], dtype='object')\n",
      "\n",
      "Model loaded successfully. Config:\n",
      "  Embedding model: sentence-transformers/all-MiniLM-L6-v2\n",
      "  Book: dorian\n",
      "  Chunks file loaded successfully: ../data/interim/chunks/dorian_chunks.json\n",
      "Ready to test retrieval\n",
      ".   Top-k: 5\n"
     ]
    }
   ],
   "source": [
    "# === TODO (you code this) ===\n",
    "# Load index & metadata; test a few queries.\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from src.embed_index import load_index\n",
    "from src.retrieve import retrieve\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import faiss\n",
    "\n",
    "# 1. Load index and metadata\n",
    "# 2. Load embedding model\n",
    "# 3. Test retrieval with example queries\n",
    "in_dir = \"../data/index\"\n",
    "index, metadata_df = load_index(in_dir)\n",
    "print(\"Index loaded successfully\")\n",
    "print(\"Index info:\")\n",
    "print(f\"  Number of vectors: {index.ntotal}\")\n",
    "print(f\"  Dimension: {index.d}\")\n",
    "print(f\"  Index type: {type(index)}\")\n",
    "print(f\"  Metadata shape: {metadata_df.shape}\")\n",
    "print(f\"  Metadata columns: {metadata_df.columns}\")\n",
    "\n",
    "config = load_config()\n",
    "model = SentenceTransformer(config['embedding_model'])\n",
    "print(\"\\nModel loaded successfully. Config:\")\n",
    "print(f\"  Embedding model: {config['embedding_model']}\")\n",
    "print(f\"  Book: {config['book']}\")\n",
    "\n",
    "chunks_lookup = None\n",
    "try:\n",
    "    book_name = config['book']\n",
    "    chunks_file = Path(f\"../data/interim/chunks/{book_name}_chunks.json\")\n",
    "    if chunks_file.exists():\n",
    "        with open(chunks_file, 'r', encoding='utf-8') as f:\n",
    "            chunks_lookup = json.load(f)\n",
    "        print(f\"  Chunks file loaded successfully: {chunks_file}\")\n",
    "    else:\n",
    "        print(f\"‚ùå Chunks file not found: {chunks_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading chunks: {e}\")\n",
    "    raise\n",
    "\n",
    "def embed_query_fn(query: str) -> np.ndarray:\n",
    "    embedding = model.encode([query],normalize_embeddings=True, show_progress_bar=True)\n",
    "    embedding = np.array(embedding, dtype=np.float32)\n",
    "    faiss.normalize_L2(embedding)\n",
    "    return embedding[0]\n",
    "\n",
    "print(\"Ready to test retrieval\")\n",
    "print(f\".   Top-k: {config.get('top_k', 5)}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  chunks_lookup is a list (length: 374), converting to dict...\n",
      "‚úÖ Converted to dictionary with 374 entries\n",
      "   Sample keys: ['dorian_chunk_0', 'dorian_chunk_1', 'dorian_chunk_2']\n"
     ]
    }
   ],
   "source": [
    "# Fix chunks_lookup: Convert from list to dictionary\n",
    "# The JSON file is a list, but retrieve() needs a dict mapping chunk_id -> chunk\n",
    "\n",
    "if chunks_lookup is not None:\n",
    "    # Check if it's a list (wrong) or dict (correct)\n",
    "    if isinstance(chunks_lookup, list):\n",
    "        print(f\"‚ö†Ô∏è  chunks_lookup is a list (length: {len(chunks_lookup)}), converting to dict...\")\n",
    "        # Convert list to dictionary: chunk['id'] -> chunk\n",
    "        chunks_lookup = {chunk['id']: chunk for chunk in chunks_lookup}\n",
    "        print(f\"‚úÖ Converted to dictionary with {len(chunks_lookup)} entries\")\n",
    "        print(f\"   Sample keys: {list(chunks_lookup.keys())[:3]}\")\n",
    "    elif isinstance(chunks_lookup, dict):\n",
    "        print(f\"‚úÖ chunks_lookup is already a dictionary with {len(chunks_lookup)} entries\")\n",
    "    else:\n",
    "        print(f\"‚ùå chunks_lookup is unexpected type: {type(chunks_lookup)}\")\n",
    "else:\n",
    "    print(\"‚ùå chunks_lookup is None - need to reload chunks\")\n",
    "    # Reload and convert properly\n",
    "    book_name = config['book']\n",
    "    chunks_file = Path(f\"../data/interim/chunks/{book_name}_chunks.json\")\n",
    "    if chunks_file.exists():\n",
    "        with open(chunks_file, 'r', encoding='utf-8') as f:\n",
    "            chunks_list = json.load(f)\n",
    "            chunks_lookup = {chunk['id']: chunk for chunk in chunks_list}\n",
    "        print(f\"‚úÖ Reloaded and converted {len(chunks_lookup)} chunks to dictionary\")\n",
    "    else:\n",
    "        print(f\"‚ùå Chunks file not found: {chunks_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Better filter function to exclude TOC/header chunks\n",
    "import re\n",
    "\n",
    "def is_toc_or_header_chunk(result):\n",
    "    \"\"\"\n",
    "    Detect if a chunk is a TOC, header, or low-content chunk.\n",
    "    Returns True if it should be filtered out.\n",
    "    \"\"\"\n",
    "    text = result['text']\n",
    "    chunk_id = result.get('chunk_id', '')\n",
    "    meta = result.get('meta', {})\n",
    "    \n",
    "    # Filter out chunk 0 (usually TOC/preface)\n",
    "    if chunk_id.endswith('_chunk_0') or meta.get('para_idx_start', -1) == 0:\n",
    "        # But allow it if it has substantial content (not just TOC)\n",
    "        if 'Contents' in text and text.count('CHAPTER') > 5:\n",
    "            return True  # It's a TOC\n",
    "    \n",
    "    # Filter very short chunks\n",
    "    if len(text) < 150:\n",
    "        return True\n",
    "    \n",
    "    # Filter chunks with too many newlines (indicates headers/TOC)\n",
    "    newline_ratio = text.count('\\n') / len(text) if len(text) > 0 else 0\n",
    "    if newline_ratio > 0.15:  # More than 15% newlines\n",
    "        return True\n",
    "    \n",
    "    # Filter chunks that are mostly chapter titles\n",
    "    lines = text.split('\\n')\n",
    "    chapter_lines = [line for line in lines if 'CHAPTER' in line.upper() or re.match(r'^CHAPTER\\s+[IVX]+', line, re.IGNORECASE)]\n",
    "    if len(chapter_lines) > 3:  # More than 3 chapter title lines\n",
    "        return True\n",
    "    \n",
    "    # Filter chunks that start with title/author/contents pattern\n",
    "    first_100 = text[:100].lower()\n",
    "    if ('contents' in first_100 and 'chapter' in first_100) or \\\n",
    "       (text.startswith('The Picture of') and 'by Oscar Wilde' in first_100):\n",
    "        # Check if it's mostly TOC (many short lines)\n",
    "        short_lines = [line for line in lines[:30] if len(line.strip()) < 50]\n",
    "        if len(short_lines) > 10:  # More than 10 short lines in first 30\n",
    "            return True\n",
    "    \n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Queries & Manual Relevance Check\n",
    "\n",
    "Test with queries like:\n",
    "\n",
    "- \"How does Homer portray Achilles' anger in Book 1?\"\n",
    "- \"What does Lord Henry claim about influence on the young?\"\n",
    "- \"Where does the poem describe the shield of Achilles?\"\n",
    "\n",
    "For each query, manually judge whether the retrieved snippets are relevant. This helps validate:\n",
    "\n",
    "1. Embedding quality (semantic similarity)\n",
    "2. Chunk size appropriateness (not too fragmented, not too broad)\n",
    "3. Retrieval ranking (most relevant chunks appear first)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6af316e585f74e359630dc9486d45428",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing filter on current results...\n",
      "\n",
      "Result 1 (dorian_chunk_349): ‚úÖ KEEP\n",
      "\n",
      "Result 2 (dorian_chunk_366): ‚úÖ KEEP\n",
      "\n",
      "Result 3 (dorian_chunk_290): ‚úÖ KEEP\n",
      "\n",
      "Result 4 (dorian_chunk_29): ‚úÖ KEEP\n",
      "\n",
      "Result 5 (dorian_chunk_56): ‚úÖ KEEP\n",
      "\n",
      "\n",
      "üìä Filtering Results:\n",
      "   Original: 5 chunks\n",
      "   Filtered: 5 chunks\n",
      "   Removed: 0 chunks\n",
      "\n",
      "Result 1:\n",
      "  Score: 0.5828\n",
      "  Chunk ID: dorian_chunk_349\n",
      "  Book: dorian\n",
      "  Paragraph range: 1449-1454\n",
      "  Text: CHAPTER XIX.\n",
      "\n",
      "‚ÄúThere is no use your telling me that you are going to be good,‚Äù cried\n",
      "Lord Henry, dipping his white fingers into a red copper bowl filled\n",
      "with rose-water. ‚ÄúYou are quite perfect. Pray, don‚Äôt change.‚Äù\n",
      "\n",
      "Dorian Gray shook his head. ‚ÄúNo, Harry, I have done too many dreadful\n",
      "things in my l...\n",
      "\n",
      "Result 2:\n",
      "  Score: 0.5699\n",
      "  Chunk ID: dorian_chunk_366\n",
      "  Book: dorian\n",
      "  Paragraph range: 1506-1508\n",
      "  Text: When he reached home, he found his servant waiting up for him. He sent\n",
      "him to bed, and threw himself down on the sofa in the library, and\n",
      "began to think over some of the things that Lord Henry had said to him.\n",
      "\n",
      "Was it really true that one could never change? He felt a wild longing\n",
      "for the unstained ...\n",
      "\n",
      "Result 3:\n",
      "  Score: 0.5282\n",
      "  Chunk ID: dorian_chunk_290\n",
      "  Book: dorian\n",
      "  Paragraph range: 1105-1112\n",
      "  Text: ‚ÄúFour husbands! Upon my word that is _trop de z√™le_.‚Äù\n",
      "\n",
      "‚Äú_Trop d‚Äôaudace_, I tell her,‚Äù said Dorian.\n",
      "\n",
      "‚ÄúOh! she is audacious enough for anything, my dear. And what is Ferrol\n",
      "like? I don‚Äôt know him.‚Äù\n",
      "\n",
      "‚ÄúThe husbands of very beautiful women belong to the criminal classes,‚Äù\n",
      "said Lord Henry, sipping his win...\n",
      "\n",
      "Result 4:\n",
      "  Score: 0.4910\n",
      "  Chunk ID: dorian_chunk_29\n",
      "  Book: dorian\n",
      "  Paragraph range: 116-119\n",
      "  Text: Dorian Gray stepped up on the dais with the air of a young Greek\n",
      "martyr, and made a little _moue_ of discontent to Lord Henry, to whom\n",
      "he had rather taken a fancy. He was so unlike Basil. They made a\n",
      "delightful contrast. And he had such a beautiful voice. After a few\n",
      "moments he said to him, ‚ÄúHave yo...\n",
      "\n",
      "Result 5:\n",
      "  Score: 0.4623\n",
      "  Chunk ID: dorian_chunk_56\n",
      "  Book: dorian\n",
      "  Paragraph range: 243-245\n",
      "  Text: ‚ÄúHe is very good-looking,‚Äù assented Lord Henry.\n",
      "\n",
      "‚ÄúI hope he will fall into proper hands,‚Äù continued the old man. ‚ÄúHe\n",
      "should have a pot of money waiting for him if Kelso did the right thing\n",
      "by him. His mother had money, too. All the Selby property came to her,\n",
      "through her grandfather. Her grandfather...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cae2ae64c7a04452835a70f961effab3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Result 1:\n",
      "  Score: 0.5341\n",
      "  Chunk ID: dorian_chunk_9\n",
      "  Book: dorian\n",
      "  Paragraph range: 34-40\n",
      "  Text: ‚ÄúWhat is that?‚Äù said the painter, keeping his eyes fixed on the ground.\n",
      "\n",
      "‚ÄúYou know quite well.‚Äù\n",
      "\n",
      "‚ÄúI do not, Harry.‚Äù\n",
      "\n",
      "‚ÄúWell, I will tell you what it is. I want you to explain to me why you\n",
      "won‚Äôt exhibit Dorian Gray‚Äôs picture. I want the real reason.‚Äù\n",
      "\n",
      "‚ÄúI told you the real reason.‚Äù\n",
      "\n",
      "‚ÄúNo, you did not. ...\n",
      "\n",
      "Result 2:\n",
      "  Score: 0.5257\n",
      "  Chunk ID: dorian_chunk_3\n",
      "  Book: dorian\n",
      "  Paragraph range: 16-18\n",
      "  Text: In the centre of the room, clamped to an upright easel, stood the\n",
      "full-length portrait of a young man of extraordinary personal beauty,\n",
      "and in front of it, some little distance away, was sitting the artist\n",
      "himself, Basil Hallward, whose sudden disappearance some years ago\n",
      "caused, at the time, such p...\n",
      "\n",
      "Result 3:\n",
      "  Score: 0.5127\n",
      "  Chunk ID: dorian_chunk_17\n",
      "  Book: dorian\n",
      "  Paragraph range: 67-69\n",
      "  Text: ‚ÄúEvery day. I couldn‚Äôt be happy if I didn‚Äôt see him every day. He is\n",
      "absolutely necessary to me.‚Äù\n",
      "\n",
      "‚ÄúHow extraordinary! I thought you would never care for anything but\n",
      "your art.‚Äù\n",
      "\n",
      "‚ÄúHe is all my art to me now,‚Äù said the painter gravely. ‚ÄúI sometimes\n",
      "think, Harry, that there are only two eras of any im...\n",
      "\n",
      "Result 4:\n",
      "  Score: 0.5110\n",
      "  Chunk ID: dorian_chunk_44\n",
      "  Book: dorian\n",
      "  Paragraph range: 175-181\n",
      "  Text: ‚ÄúI am jealous of everything whose beauty does not die. I am jealous of\n",
      "the portrait you have painted of me. Why should it keep what I must\n",
      "lose? Every moment that passes takes something from me and gives\n",
      "something to it. Oh, if it were only the other way! If the picture\n",
      "could change, and I could be ...\n",
      "\n",
      "Result 5:\n",
      "  Score: 0.5024\n",
      "  Chunk ID: dorian_chunk_43\n",
      "  Book: dorian\n",
      "  Paragraph range: 174-176\n",
      "  Text: Hallward turned pale and caught his hand. ‚ÄúDorian! Dorian!‚Äù he cried,\n",
      "‚Äúdon‚Äôt talk like that. I have never had such a friend as you, and I\n",
      "shall never have such another. You are not jealous of material things,\n",
      "are you?‚Äîyou who are finer than any of them!‚Äù\n",
      "\n",
      "‚ÄúI am jealous of everything whose beauty doe...\n"
     ]
    }
   ],
   "source": [
    "# Test queries and display top-k results\n",
    "# For each query, show:\n",
    "# - Query text\n",
    "# - Top 3-5 retrieved chunks with scores\n",
    "# - Manual relevance judgment (relevant/partially relevant/not relevant)\n",
    "\n",
    "query = \"Lord Henry says all influence is immoral\"\n",
    "\n",
    "results = retrieve(\n",
    "    query=query, \n",
    "    index=index, \n",
    "    embed_fn=embed_query_fn, \n",
    "    metadata_df=metadata_df, \n",
    "    chunks_lookup=chunks_lookup,\n",
    "    k=config.get('top_k', 5)\n",
    ")\n",
    "\n",
    "# After retrieval, filter out low-content chunks\n",
    "print(\"Testing filter on current results...\\n\")\n",
    "\n",
    "for i, result in enumerate(results, 1):\n",
    "    should_filter = is_toc_or_header_chunk(result)\n",
    "    status = \"‚ùå FILTER OUT\" if should_filter else \"‚úÖ KEEP\"\n",
    "    print(f\"Result {i} ({result['chunk_id']}): {status}\")\n",
    "    if should_filter:\n",
    "        print(f\"  Reason: TOC/header detected\")\n",
    "    print()\n",
    "\n",
    "# Apply the filter\n",
    "filtered_results = [r for r in results if not is_toc_or_header_chunk(r)]\n",
    "\n",
    "print(f\"\\nüìä Filtering Results:\")\n",
    "print(f\"   Original: {len(results)} chunks\")\n",
    "print(f\"   Filtered: {len(filtered_results)} chunks\")\n",
    "print(f\"   Removed: {len(results) - len(filtered_results)} chunks\")\n",
    "\n",
    "for i, result in enumerate(filtered_results, 1):\n",
    "    print(f\"\\nResult {i}:\")\n",
    "    print(f\"  Score: {result['score']:.4f}\")\n",
    "    print(f\"  Chunk ID: {result['chunk_id']}\")\n",
    "    print(f\"  Book: {result['meta']['book']}\")\n",
    "    print(f\"  Paragraph range: {result['meta']['para_idx_start']}-{result['meta']['para_idx_end']}\")\n",
    "    print(f\"  Text: {result['text'][:300]}...\")\n",
    "\n",
    "query_2 = \"Describe the appearance of the portrait painting of the young man\"\n",
    "\n",
    "results_2 = retrieve(\n",
    "    query=query_2, \n",
    "    index=index, \n",
    "    embed_fn=embed_query_fn, \n",
    "    metadata_df=metadata_df, \n",
    "    chunks_lookup=chunks_lookup,\n",
    "    k=config.get('top_k', 5)\n",
    ")\n",
    "\n",
    "# After retrieval, filter out low-content chunks\n",
    "filtered_results_2 = []\n",
    "\n",
    "for result in results_2:\n",
    "    text = result['text']\n",
    "    # Skip if it's mostly headers/TOC (lots of all caps, short lines, etc.)\n",
    "    if len(text) < 200 or text.count('\\n') / len(text) > 0.1:\n",
    "        continue\n",
    "    filtered_results_2.append(result)\n",
    "\n",
    "for i, result in enumerate(filtered_results_2, 1):\n",
    "    print(f\"\\nResult {i}:\")\n",
    "    print(f\"  Score: {result['score']:.4f}\")\n",
    "    print(f\"  Chunk ID: {result['chunk_id']}\")\n",
    "    print(f\"  Book: {result['meta']['book']}\")\n",
    "    print(f\"  Paragraph range: {result['meta']['para_idx_start']}-{result['meta']['para_idx_end']}\")\n",
    "    print(f\"  Text: {result['text'][:300]}...\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "At this point, you should have:\n",
    "\n",
    "- ‚úÖ Full FAISS index built and saved to `data/index/`\n",
    "- ‚úÖ Metadata persisted alongside the index\n",
    "- ‚úÖ Retrieval tested with example queries\n",
    "- ‚úÖ Manual validation that retrieved chunks are relevant\n",
    "\n",
    "**Next notebook**: Build a small QA evaluation set, test answer composition, and wire up the Gradio demo.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Codecademy ML",
   "language": "python",
   "name": "codeacademy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
