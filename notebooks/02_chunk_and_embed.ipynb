{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 02: Chunking Strategy & Embedding Preview\n",
        "\n",
        "## Goal\n",
        "\n",
        "Split cleaned text into overlapping chunks, then generate embeddings for a sample to verify the pipeline.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Chunking Strategy Trade-offs\n",
        "\n",
        "### Size\n",
        "\n",
        "- **Too small** (< 400 chars): Loses context, may break sentences\n",
        "- **Too large** (> 1200 chars): Dilutes relevance, harder to retrieve precise passages\n",
        "- **Sweet spot**: ~800 characters balances context and precision\n",
        "\n",
        "### Overlap\n",
        "\n",
        "- **No overlap**: Risk of splitting important passages across chunk boundaries\n",
        "- **Large overlap** (> 50%): Redundant storage, slower retrieval\n",
        "- **Moderate overlap** (~15%): Preserves continuity without excessive redundancy\n",
        "\n",
        "### Our Approach\n",
        "\n",
        "1. Split into paragraphs first (preserve natural boundaries)\n",
        "2. Accumulate paragraphs until we reach `chunk_size` characters\n",
        "3. Slide by `chunk_size - chunk_overlap` to create overlapping windows\n",
        "4. Attach metadata: book name, paragraph indices, character span\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why sentence-transformers MiniLM?\n",
        "\n",
        "We use `sentence-transformers/all-MiniLM-L6-v2` because:\n",
        "\n",
        "- **Speed**: Fast encoding on CPU (no GPU required)\n",
        "- **Quality**: Good semantic similarity for literary text\n",
        "- **Size**: Small model (~80MB), easy to load\n",
        "- **Normalization**: Supports normalized embeddings for cosine similarity / inner product in FAISS\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Normalization for FAISS\n",
        "\n",
        "FAISS supports two distance metrics:\n",
        "\n",
        "- **L2 (Euclidean)**: Requires no normalization\n",
        "- **Inner Product (IP)**: Requires normalized vectors (unit length)\n",
        "\n",
        "We'll use **Inner Product** with normalized embeddings because:\n",
        "\n",
        "- Equivalent to cosine similarity for normalized vectors\n",
        "- Faster search in FAISS for normalized vectors\n",
        "- Better semantic matching for text\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load Cleaned Text\n",
        "\n",
        "Load the cleaned text from the previous notebook.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load cleaned text from data/interim/\n",
        "# Use the book name from config to construct the file path.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Split into Paragraphs\n",
        "\n",
        "Split the cleaned text into paragraphs (double newline boundaries).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Split cleaned text into paragraphs.\n",
        "# Acceptance: list[str], len>100 for full books.\n",
        "\n",
        "from src.chunk import split_into_paragraphs\n",
        "\n",
        "# Call split_into_paragraphs and verify the result.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Create Overlapping Chunks\n",
        "\n",
        "Chunk paragraphs into fixed-size overlapping segments with metadata.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Chunk paragraphs into fixed-size overlapping chunks with metadata.\n",
        "# Acceptance: list[dict] with 'id','text','meta'.\n",
        "\n",
        "from src.chunk import chunk_paragraphs\n",
        "\n",
        "# Use chunk_size and chunk_overlap from config.\n",
        "# Verify chunk structure: each should have id, text, and metadata.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Embed Sample Chunks\n",
        "\n",
        "Generate embeddings for a small sample to verify shape and variance.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Embed a small sample of chunks to sanity-check shape and variance.\n",
        "# Acceptance: embedding array shape == (n_sample, d).\n",
        "\n",
        "from src.embed_index import embed_texts\n",
        "\n",
        "# Take first 10 chunks, embed them, check shape and maybe compute pairwise similarity.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Optional: 2D Projection Visualization\n",
        "\n",
        "Use PCA or t-SNE to project embeddings to 2D and visualize chunk similarity. This helps verify that semantically similar chunks cluster together.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: 2D projection of embeddings\n",
        "# from sklearn.manifold import TSNE\n",
        "# import matplotlib.pyplot as plt\n",
        "# \n",
        "# # Project and plot embeddings\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Token Coverage vs. Character Chunks\n",
        "\n",
        "**Note**: We're using character-based chunking (not token-based). This is simpler but may split tokens in some edge cases. For production, consider token-aware chunking (e.g., using a tokenizer to respect word boundaries).\n",
        "\n",
        "For this project, character chunks work well because:\n",
        "\n",
        "- Literary text has consistent word boundaries\n",
        "- Simpler implementation\n",
        "- Overlap mitigates boundary issues\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "At this point, you should have:\n",
        "\n",
        "- ✅ Paragraphs split from cleaned text\n",
        "- ✅ Overlapping chunks with metadata\n",
        "- ✅ Sample embeddings verified (correct shape, reasonable variance)\n",
        "\n",
        "**Next notebook**: Build the full FAISS index and test retrieval.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
