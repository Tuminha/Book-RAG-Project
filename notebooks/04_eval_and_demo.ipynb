{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 04: Evaluation & Gradio Demo\n",
        "\n",
        "## Goal\n",
        "\n",
        "Build a lightweight evaluation set, test answer composition with verbatim quotes, and wire up a Gradio demo.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building a Tiny Gold QA Set\n",
        "\n",
        "Create 10â€“20 question-answer pairs manually:\n",
        "\n",
        "- **Questions**: Focused, answerable from the text (e.g., \"What does Lord Henry say about influence?\")\n",
        "- **Acceptable answer keywords**: Terms that should appear in retrieved chunks (e.g., \"influence\", \"young\", \"soul\")\n",
        "- **Notes**: Optional context about expected answer structure\n",
        "\n",
        "Save this as `data/interim/qa_dev.csv` with columns: `question`, `acceptable_answer_keywords`, `notes`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Metrics: Recall@k & Groundedness\n",
        "\n",
        "### Recall@k (Retrieval)\n",
        "\n",
        "For each question, check if at least one retrieved chunk (top-k) contains any of the acceptable answer keywords.\n",
        "\n",
        "- **Recall@5**: Proportion of questions where a gold-supporting chunk appears in top-5\n",
        "- **Target**: â‰¥ 0.8 (80% of questions have relevant chunks retrieved)\n",
        "\n",
        "### Groundedness (Composition)\n",
        "\n",
        "Measure how well answers are grounded in quotes:\n",
        "\n",
        "- **Quote presence**: % of answers with â‰¥1 quote\n",
        "- **Attribution score**: Mean fraction of answer sentences that share â‰¥2 content words with some quote\n",
        "- **Target**: Groundedness â‰¥ 0.95, Attribution â‰¥ 0.7\n",
        "\n",
        "### Latency (UX)\n",
        "\n",
        "Mean retrieval + compose time on CPU for one query. Target: < 2 seconds.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Answer Style Guide\n",
        "\n",
        "When composing answers:\n",
        "\n",
        "- **Length**: 2â€“4 sentences, ~100â€“140 words\n",
        "- **Tone**: Assertive but qualified (\"the text suggestsâ€¦\", \"the narrator framesâ€¦\")\n",
        "- **No inventions**: Every factual clause must be traceable to a quote\n",
        "- **References**: Use [1], [2], [3] in the answer; match to citations list\n",
        "- **Quote selection**: Prefer one quote that defines, one that illustrates, and one that contrasts (when available)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Query Length Flexibility\n",
        "\n",
        "**Important**: Queries do NOT need to be the same length!\n",
        "\n",
        "The embedding model (`sentence-transformers/all-MiniLM-L6-v2`) can handle queries of **any length**:\n",
        "- Short queries (1-5 words): \"Who is Basil?\"\n",
        "- Medium queries (6-15 words): \"What does Lord Henry say about beauty?\"\n",
        "- Long queries (16+ words): \"What does Lord Henry claim about influence on young people and how does he explain his philosophy?\"\n",
        "\n",
        "The model automatically:\n",
        "1. Tokenizes the query (handles variable-length text)\n",
        "2. Generates a fixed-size embedding (384 dimensions)\n",
        "3. Normalizes the embedding for semantic search\n",
        "\n",
        "**Best Practice**: Write queries naturally - use the length that best expresses your question. Longer queries can be more specific, but shorter queries often work well too!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Create QA Development Set\n",
        "\n",
        "Manually create a small QA dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Created QA development set with 10 questions\n",
            "   Saved to: ../data/interim/qa_dev.csv\n",
            "\n",
            "ðŸ“Š Query length distribution:\n",
            "   Q1: 6 words - \"What does the portrait look like?\"\n",
            "   Q2: 4 words - \"Who is Basil Hallward?\"\n",
            "   Q3: 3 words - \"Describe Dorian Gray.\"\n",
            "   Q4: 9 words - \"What does Lord Henry say about beauty and intellec...\"\n",
            "   Q5: 8 words - \"Why doesn't Basil want to exhibit the portrait?\"\n",
            "   Q6: 10 words - \"How does Basil describe meeting Dorian for the fir...\"\n",
            "   Q7: 9 words - \"What happens to the portrait as the story progress...\"\n",
            "   Q8: 17 words - \"What does Lord Henry claim about influence on youn...\"\n",
            "   Q9: 13 words - \"How does the story describe the relationship betwe...\"\n",
            "   Q10: 17 words - \"What are Lord Henry's views on art, beauty, and th...\"\n",
            "\n",
            "ðŸ’¡ Note: Queries vary from 3 to 17 words - all work fine!\n"
          ]
        }
      ],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Create a small QA dataframe: {question, acceptable_answer_keywords, notes}.\n",
        "# Acceptance: CSV written to data/interim/qa_dev.csv\n",
        "\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Create diverse QA pairs with varying lengths and question types\n",
        "# Note: Queries can be ANY length - the embedding model handles variable-length text!\n",
        "\n",
        "# Short queries (1-5 words)\n",
        "query_1 = \"What does the portrait look like?\"\n",
        "query_2 = \"Who is Basil Hallward?\"\n",
        "query_3 = \"Describe Dorian Gray.\"\n",
        "\n",
        "# Medium queries (6-10 words)\n",
        "query_4 = \"What does Lord Henry say about beauty and intellect?\"\n",
        "query_5 = \"Why doesn't Basil want to exhibit the portrait?\"\n",
        "query_6 = \"How does Basil describe meeting Dorian for the first time?\"\n",
        "query_7 = \"What happens to the portrait as the story progresses?\"\n",
        "\n",
        "# Long queries (11+ words)\n",
        "query_8 = \"What does Lord Henry claim about influence on young people and how does he explain his philosophy?\"\n",
        "query_9 = \"How does the story describe the relationship between Dorian Gray and his portrait?\"\n",
        "query_10 = \"What are Lord Henry's views on art, beauty, and the purpose of life according to the text?\"\n",
        "\n",
        "# Create list of queries (note: they vary significantly in length!)\n",
        "queries = [\n",
        "    query_1, query_2, query_3, query_4, query_5,\n",
        "    query_6, query_7, query_8, query_9, query_10\n",
        "]\n",
        "\n",
        "# Define acceptable answer keywords for each query\n",
        "keywords = [\n",
        "    \"portrait, painting, young man, beauty\",  # query_1\n",
        "    \"Basil Hallward, artist, painter\",  # query_2\n",
        "    \"Dorian Gray, young, beautiful, handsome\",  # query_3\n",
        "    \"beauty, intellect, intellectual expression, harmony\",  # query_4\n",
        "    \"exhibit, portrait, too much, himself\",  # query_5\n",
        "    \"Basil, meeting, Dorian, first time, Lady Brandon\",  # query_6\n",
        "    \"portrait, changes, ages, corruption\",  # query_7\n",
        "    \"influence, immoral, soul, self-development, nature\",  # query_8\n",
        "    \"portrait, relationship, mirror, reflection, corruption\",  # query_9\n",
        "    \"art, beauty, life, purpose, philosophy, Lord Henry\"  # query_10\n",
        "]\n",
        "\n",
        "# Create notes for context\n",
        "notes = [\n",
        "    \"Should retrieve description of the portrait from early chapters\",\n",
        "    \"Character introduction - should be in first few chapters\",\n",
        "    \"Physical description of Dorian - early in book\",\n",
        "    \"Lord Henry's philosophy about beauty vs intellect\",\n",
        "    \"Basil's reason for not exhibiting - early conversation\",\n",
        "    \"Basil's story about meeting Dorian at Lady Brandon's party\",\n",
        "    \"Portrait's transformation - later in book\",\n",
        "    \"Lord Henry's famous speech about influence - Chapter 2\",\n",
        "    \"Central theme - portrait as mirror of soul\",\n",
        "    \"Lord Henry's aesthetic philosophy throughout the book\"\n",
        "]\n",
        "\n",
        "# Create dataframe\n",
        "qa_dev = pd.DataFrame({\n",
        "    'question': queries,\n",
        "    'acceptable_answer_keywords': keywords,\n",
        "    'notes': notes\n",
        "})\n",
        "\n",
        "# Save to CSV\n",
        "output_dir = Path(\"../data/interim\")\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "output_path = output_dir / \"qa_dev.csv\"\n",
        "qa_dev.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"âœ… Created QA development set with {len(queries)} questions\")\n",
        "print(f\"   Saved to: {output_path}\")\n",
        "print(f\"\\nðŸ“Š Query length distribution:\")\n",
        "for i, q in enumerate(queries, 1):\n",
        "    word_count = len(q.split())\n",
        "    print(f\"   Q{i}: {word_count} words - \\\"{q[:50]}{'...' if len(q) > 50 else ''}\\\"\")\n",
        "print(f\"\\nðŸ’¡ Note: Queries vary from {min(len(q.split()) for q in queries)} to {max(len(q.split()) for q in queries)} words - all work fine!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Evaluate Retrieval (Recall@k)\n",
        "\n",
        "For each question, retrieve top-k chunks and check if any contain the acceptable keywords.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Loaded index: 374 vectors, dimension 384\n",
            "âœ… Loaded metadata: 374 rows\n",
            "\n",
            "âœ… Recall@5: 100.00% (10/10 questions)\n",
            "\n",
            "================================================================================\n",
            "ðŸ“Š Detailed Results:\n",
            "================================================================================\n",
            "\n",
            "Q1: âœ… PASS\n",
            "   Question: What does the portrait look like?\n",
            "   âœ… Matched at rank 1 (chunk: dorian_chunk_9)\n",
            "   âœ… Matched keywords: portrait\n",
            "   ðŸ“ All keywords: portrait, painting, young man, beauty\n",
            "\n",
            "Q2: âœ… PASS\n",
            "   Question: Who is Basil Hallward?\n",
            "   âœ… Matched at rank 1 (chunk: dorian_chunk_40)\n",
            "   âœ… Matched keywords: basil hallward, painter\n",
            "   ðŸ“ All keywords: basil hallward, artist, painter\n",
            "\n",
            "Q3: âœ… PASS\n",
            "   Question: Describe Dorian Gray.\n",
            "   âœ… Matched at rank 1 (chunk: dorian_chunk_35)\n",
            "   âœ… Matched keywords: dorian gray, young\n",
            "   ðŸ“ All keywords: dorian gray, young, beautiful, handsome\n",
            "\n",
            "Q4: âœ… PASS\n",
            "   Question: What does Lord Henry say about beauty and intellect?\n",
            "   âœ… Matched at rank 5 (chunk: dorian_chunk_5)\n",
            "   âœ… Matched keywords: beauty, intellect, intellectual expression, harmony\n",
            "   ðŸ“ All keywords: beauty, intellect, intellectual expression, harmony\n",
            "\n",
            "Q5: âœ… PASS\n",
            "   Question: Why doesn't Basil want to exhibit the portrait?\n",
            "   âœ… Matched at rank 1 (chunk: dorian_chunk_18)\n",
            "   âœ… Matched keywords: exhibit, portrait, too much\n",
            "   ðŸ“ All keywords: exhibit, portrait, too much, himself\n",
            "\n",
            "Q6: âœ… PASS\n",
            "   Question: How does Basil describe meeting Dorian for the first time?\n",
            "   âœ… Matched at rank 1 (chunk: dorian_chunk_245)\n",
            "   âœ… Matched keywords: basil, dorian\n",
            "   ðŸ“ All keywords: basil, meeting, dorian, first time, lady brandon\n",
            "\n",
            "Q7: âœ… PASS\n",
            "   Question: What happens to the portrait as the story progresses?\n",
            "   âœ… Matched at rank 1 (chunk: dorian_chunk_176)\n",
            "   âœ… Matched keywords: portrait\n",
            "   ðŸ“ All keywords: portrait, changes, ages, corruption\n",
            "\n",
            "Q8: âœ… PASS\n",
            "   Question: What does Lord Henry claim about influence on young people and how does he explain his philosophy?\n",
            "   âœ… Matched at rank 1 (chunk: dorian_chunk_366)\n",
            "   âœ… Matched keywords: influence\n",
            "   ðŸ“ All keywords: influence, immoral, soul, self-development, nature\n",
            "\n",
            "Q9: âœ… PASS\n",
            "   Question: How does the story describe the relationship between Dorian Gray and his portrait?\n",
            "   âœ… Matched at rank 1 (chunk: dorian_chunk_60)\n",
            "   âœ… Matched keywords: portrait\n",
            "   ðŸ“ All keywords: portrait, relationship, mirror, reflection, corruption\n",
            "\n",
            "Q10: âœ… PASS\n",
            "   Question: What are Lord Henry's views on art, beauty, and the purpose of life according to the text?\n",
            "   âœ… Matched at rank 1 (chunk: dorian_chunk_366)\n",
            "   âœ… Matched keywords: life, lord henry\n",
            "   ðŸ“ All keywords: art, beauty, life, purpose, philosophy, lord henry\n",
            "\n",
            "================================================================================\n",
            "ðŸ’¡ Analysis:\n",
            "   - 10/10 questions have at least one keyword match in top-5 chunks\n",
            "   - Average rank of first match: 1.4\n",
            "   - Questions with match at rank 1: 9/10\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Evaluate retrieval: for each Q, if any retrieved chunk contains a keyword â†’ hit.\n",
        "# Acceptance: print Recall@k summary.\n",
        "\n",
        "import sys\n",
        "sys.path.append('..')\n",
        "from src.retrieve import retrieve\n",
        "\n",
        "# Load QA set\n",
        "qa_dev = pd.read_csv('../data/interim/qa_dev.csv')\n",
        "\n",
        "# Load index and model for retrieval\n",
        "from src.embed_index import load_index\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import faiss\n",
        "\n",
        "index, metadata_df = load_index('../data/index')\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Load chunks lookup\n",
        "import json\n",
        "with open('../data/interim/chunks/dorian_chunks.json', 'r', encoding='utf-8') as f:\n",
        "    chunks_list = json.load(f)\n",
        "    chunks_lookup = {chunk['id']: chunk for chunk in chunks_list}\n",
        "\n",
        "def embed_query(query: str) -> np.ndarray:\n",
        "    \"\"\"Embed query and normalize for retrieval.\"\"\"\n",
        "    embedding = model.encode([query], normalize_embeddings=True, show_progress_bar=False)\n",
        "    embedding = np.array(embedding, dtype=np.float32)\n",
        "    faiss.normalize_L2(embedding)\n",
        "    return embedding[0]\n",
        "\n",
        "# Evaluate Recall@k with detailed diagnostics\n",
        "k = 5\n",
        "hits = 0\n",
        "total = len(qa_dev)\n",
        "results_detail = []\n",
        "\n",
        "for i, row in qa_dev.iterrows():\n",
        "    question = row['question']\n",
        "    keywords_str = row['acceptable_answer_keywords']\n",
        "    keywords = [kw.strip().lower() for kw in keywords_str.split(',')]\n",
        "    \n",
        "    # Retrieve top-k chunks\n",
        "    results = retrieve(\n",
        "        query=question,\n",
        "        index=index,\n",
        "        embed_fn=embed_query,\n",
        "        metadata_df=metadata_df,\n",
        "        chunks_lookup=chunks_lookup,\n",
        "        k=k\n",
        "    )\n",
        "    \n",
        "    # Check if any chunk contains acceptable keywords\n",
        "    hit = False\n",
        "    matched_chunk_rank = None\n",
        "    matched_keywords = []\n",
        "    matched_chunk_id = None\n",
        "    \n",
        "    for rank, result in enumerate(results, 1):\n",
        "        text = result.get('text', '').lower()\n",
        "        chunk_id = result.get('chunk_id', 'unknown')\n",
        "        \n",
        "        # Check which keywords match in this chunk\n",
        "        found_keywords = [kw for kw in keywords if kw in text]\n",
        "        \n",
        "        if found_keywords:\n",
        "            hit = True\n",
        "            matched_chunk_rank = rank\n",
        "            matched_keywords = found_keywords\n",
        "            matched_chunk_id = chunk_id\n",
        "            break\n",
        "    \n",
        "    results_detail.append({\n",
        "        'question': question,\n",
        "        'hit': hit,\n",
        "        'rank': matched_chunk_rank,\n",
        "        'matched_keywords': matched_keywords,\n",
        "        'chunk_id': matched_chunk_id,\n",
        "        'all_keywords': keywords\n",
        "    })\n",
        "    \n",
        "    if hit:\n",
        "        hits += 1\n",
        "\n",
        "recall_at_k = hits / total\n",
        "print(f\"\\nâœ… Recall@{k}: {recall_at_k:.2%} ({hits}/{total} questions)\\n\")\n",
        "\n",
        "# Print detailed diagnostics\n",
        "print(\"=\" * 80)\n",
        "print(\"ðŸ“Š Detailed Results:\")\n",
        "print(\"=\" * 80)\n",
        "for i, detail in enumerate(results_detail, 1):\n",
        "    status = \"âœ… PASS\" if detail['hit'] else \"âŒ FAIL\"\n",
        "    print(f\"\\nQ{i}: {status}\")\n",
        "    print(f\"   Question: {detail['question']}\")\n",
        "    if detail['hit']:\n",
        "        print(f\"   âœ… Matched at rank {detail['rank']} (chunk: {detail['chunk_id']})\")\n",
        "        print(f\"   âœ… Matched keywords: {', '.join(detail['matched_keywords'])}\")\n",
        "        print(f\"   ðŸ“ All keywords: {', '.join(detail['all_keywords'])}\")\n",
        "    else:\n",
        "        print(f\"   âŒ No keywords found in top-{k} results\")\n",
        "        print(f\"   ðŸ“ Expected keywords: {', '.join(detail['all_keywords'])}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(f\"ðŸ’¡ Analysis:\")\n",
        "print(f\"   - {hits}/{total} questions have at least one keyword match in top-{k} chunks\")\n",
        "print(f\"   - Average rank of first match: {sum(d['rank'] for d in results_detail if d['hit']) / hits:.1f}\" if hits > 0 else \"   - No matches found\")\n",
        "print(f\"   - Questions with match at rank 1: {sum(1 for d in results_detail if d['hit'] and d['rank'] == 1)}/{hits}\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Compose Answers with Quotes\n",
        "\n",
        "Test the answer composition pipeline with retrieved chunks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Compose an answer from retrieved chunks (no LLM), with quotes and citations.\n",
        "# Acceptance: dict with 'answer', 'quotes', 'used_chunks'.\n",
        "\n",
        "from src.compose import compose_answer\n",
        "\n",
        "# Test on a few example queries\n",
        "# Verify that answers include quotes and citations\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Evaluate Groundedness\n",
        "\n",
        "Measure how well answers are grounded in quotes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Evaluate \"groundedness\": % of answers containing â‰¥1 quote AND each claim sentence overlaps tokens with at least one quote.\n",
        "# Hints:\n",
        "# 1) For a small QA set, run retrieve -> compose; check non-empty quotes.\n",
        "# 2) Token-overlap heuristic: for each answer sentence, require >= t shared content words with some quote (t ~ 2â€“3).\n",
        "# Acceptance:\n",
        "# - Print groundedness rate and sample diagnostics for 3 questions.\n",
        "\n",
        "# For each question:\n",
        "#   - Retrieve and compose answer\n",
        "#   - Check quote presence\n",
        "#   - Compute token overlap between answer sentences and quotes\n",
        "# Report groundedness and attribution scores\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Wire Gradio Demo\n",
        "\n",
        "Create a simple Gradio interface for interactive Q&A.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'src' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Force reload the app\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimportlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m reload\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m reload(\u001b[43msrc\u001b[49m.app)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Launch the demo\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Test with a few questions interactively\u001b[39;00m\n\u001b[32m     14\u001b[39m demo = launch_app()\n",
            "\u001b[31mNameError\u001b[39m: name 'src' is not defined"
          ]
        }
      ],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Wire a simple Gradio demo using src/app.launch_app().\n",
        "\n",
        "\n",
        "\n",
        "# Force reload the app\n",
        "\n",
        "\n",
        "# Launch the demo\n",
        "# Test with a few questions interactively\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Prepare for Hugging Face Spaces Deployment\n",
        "\n",
        "Optional: Prepare the app for deployment to Hugging Face Spaces.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Prepare for Hugging Face Spaces deployment.\n",
        "# Hints:\n",
        "# 1) Ensure app entrypoint is src/app.py with a function `launch_app()` or `demo = gr.Interface(...)`.\n",
        "# 2) Create a `README.md` for the Space (use SPACE_CARD.md text).\n",
        "# 3) Runtime: set \"Hardware: CPU basic\", \"SDK: Gradio\", \"Space Timeout: 120s\".\n",
        "# Acceptance:\n",
        "# - Space builds successfully; interacts within ~2â€“5 seconds per query on CPU.\n",
        "\n",
        "# See SPACE_CARD.md for the README content to use in the Space.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "At this point, you should have:\n",
        "\n",
        "- âœ… QA development set created (`data/interim/qa_dev.csv`)\n",
        "- âœ… Recall@k evaluated (target: â‰¥ 0.8)\n",
        "- âœ… Groundedness evaluated (target: â‰¥ 0.95)\n",
        "- âœ… Answer composition tested with quotes and citations\n",
        "- âœ… Gradio demo working locally\n",
        "- âœ… (Optional) Space deployment ready\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "- Optional LLM rewrite step (keeps quotes, improves fluency)\n",
        "- Named-entity & character graph for richer answers\n",
        "- Multi-book corpus with per-source filtering\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Codecademy ML",
      "language": "python",
      "name": "codeacademy"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
