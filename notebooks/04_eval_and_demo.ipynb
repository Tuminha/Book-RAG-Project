{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 04: Evaluation & Gradio Demo\n",
        "\n",
        "## Goal\n",
        "\n",
        "Build a lightweight evaluation set, test answer composition with verbatim quotes, and wire up a Gradio demo.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building a Tiny Gold QA Set\n",
        "\n",
        "Create 10â€“20 question-answer pairs manually:\n",
        "\n",
        "- **Questions**: Focused, answerable from the text (e.g., \"What does Lord Henry say about influence?\")\n",
        "- **Acceptable answer keywords**: Terms that should appear in retrieved chunks (e.g., \"influence\", \"young\", \"soul\")\n",
        "- **Notes**: Optional context about expected answer structure\n",
        "\n",
        "Save this as `data/interim/qa_dev.csv` with columns: `question`, `acceptable_answer_keywords`, `notes`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Metrics: Recall@k & Groundedness\n",
        "\n",
        "### Recall@k (Retrieval)\n",
        "\n",
        "For each question, check if at least one retrieved chunk (top-k) contains any of the acceptable answer keywords.\n",
        "\n",
        "- **Recall@5**: Proportion of questions where a gold-supporting chunk appears in top-5\n",
        "- **Target**: â‰¥ 0.8 (80% of questions have relevant chunks retrieved)\n",
        "\n",
        "### Groundedness (Composition)\n",
        "\n",
        "Measure how well answers are grounded in quotes:\n",
        "\n",
        "- **Quote presence**: % of answers with â‰¥1 quote\n",
        "- **Attribution score**: Mean fraction of answer sentences that share â‰¥2 content words with some quote\n",
        "- **Target**: Groundedness â‰¥ 0.95, Attribution â‰¥ 0.7\n",
        "\n",
        "### Latency (UX)\n",
        "\n",
        "Mean retrieval + compose time on CPU for one query. Target: < 2 seconds.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Answer Style Guide\n",
        "\n",
        "When composing answers:\n",
        "\n",
        "- **Length**: 2â€“4 sentences, ~100â€“140 words\n",
        "- **Tone**: Assertive but qualified (\"the text suggestsâ€¦\", \"the narrator framesâ€¦\")\n",
        "- **No inventions**: Every factual clause must be traceable to a quote\n",
        "- **References**: Use [1], [2], [3] in the answer; match to citations list\n",
        "- **Quote selection**: Prefer one quote that defines, one that illustrates, and one that contrasts (when available)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Query Length Flexibility\n",
        "\n",
        "**Important**: Queries do NOT need to be the same length!\n",
        "\n",
        "The embedding model (`sentence-transformers/all-MiniLM-L6-v2`) can handle queries of **any length**:\n",
        "- Short queries (1-5 words): \"Who is Basil?\"\n",
        "- Medium queries (6-15 words): \"What does Lord Henry say about beauty?\"\n",
        "- Long queries (16+ words): \"What does Lord Henry claim about influence on young people and how does he explain his philosophy?\"\n",
        "\n",
        "The model automatically:\n",
        "1. Tokenizes the query (handles variable-length text)\n",
        "2. Generates a fixed-size embedding (384 dimensions)\n",
        "3. Normalizes the embedding for semantic search\n",
        "\n",
        "**Best Practice**: Write queries naturally - use the length that best expresses your question. Longer queries can be more specific, but shorter queries often work well too!\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Create QA Development Set\n",
        "\n",
        "Manually create a small QA dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Created QA development set with 10 questions\n",
            "   Saved to: ../data/interim/qa_dev.csv\n",
            "\n",
            "ðŸ“Š Query length distribution:\n",
            "   Q1: 6 words - \"What does the portrait look like?\"\n",
            "   Q2: 4 words - \"Who is Basil Hallward?\"\n",
            "   Q3: 3 words - \"Describe Dorian Gray.\"\n",
            "   Q4: 9 words - \"What does Lord Henry say about beauty and intellec...\"\n",
            "   Q5: 8 words - \"Why doesn't Basil want to exhibit the portrait?\"\n",
            "   Q6: 10 words - \"How does Basil describe meeting Dorian for the fir...\"\n",
            "   Q7: 9 words - \"What happens to the portrait as the story progress...\"\n",
            "   Q8: 17 words - \"What does Lord Henry claim about influence on youn...\"\n",
            "   Q9: 13 words - \"How does the story describe the relationship betwe...\"\n",
            "   Q10: 17 words - \"What are Lord Henry's views on art, beauty, and th...\"\n",
            "\n",
            "ðŸ’¡ Note: Queries vary from 3 to 17 words - all work fine!\n"
          ]
        }
      ],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Create a small QA dataframe: {question, acceptable_answer_keywords, notes}.\n",
        "# Acceptance: CSV written to data/interim/qa_dev.csv\n",
        "\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Create diverse QA pairs with varying lengths and question types\n",
        "# Note: Queries can be ANY length - the embedding model handles variable-length text!\n",
        "\n",
        "# Short queries (1-5 words)\n",
        "query_1 = \"What does the portrait look like?\"\n",
        "query_2 = \"Who is Basil Hallward?\"\n",
        "query_3 = \"Describe Dorian Gray.\"\n",
        "\n",
        "# Medium queries (6-10 words)\n",
        "query_4 = \"What does Lord Henry say about beauty and intellect?\"\n",
        "query_5 = \"Why doesn't Basil want to exhibit the portrait?\"\n",
        "query_6 = \"How does Basil describe meeting Dorian for the first time?\"\n",
        "query_7 = \"What happens to the portrait as the story progresses?\"\n",
        "\n",
        "# Long queries (11+ words)\n",
        "query_8 = \"What does Lord Henry claim about influence on young people and how does he explain his philosophy?\"\n",
        "query_9 = \"How does the story describe the relationship between Dorian Gray and his portrait?\"\n",
        "query_10 = \"What are Lord Henry's views on art, beauty, and the purpose of life according to the text?\"\n",
        "\n",
        "# Create list of queries (note: they vary significantly in length!)\n",
        "queries = [\n",
        "    query_1, query_2, query_3, query_4, query_5,\n",
        "    query_6, query_7, query_8, query_9, query_10\n",
        "]\n",
        "\n",
        "# Define acceptable answer keywords for each query\n",
        "keywords = [\n",
        "    \"portrait, painting, young man, beauty\",  # query_1\n",
        "    \"Basil Hallward, artist, painter\",  # query_2\n",
        "    \"Dorian Gray, young, beautiful, handsome\",  # query_3\n",
        "    \"beauty, intellect, intellectual expression, harmony\",  # query_4\n",
        "    \"exhibit, portrait, too much, himself\",  # query_5\n",
        "    \"Basil, meeting, Dorian, first time, Lady Brandon\",  # query_6\n",
        "    \"portrait, changes, ages, corruption\",  # query_7\n",
        "    \"influence, immoral, soul, self-development, nature\",  # query_8\n",
        "    \"portrait, relationship, mirror, reflection, corruption\",  # query_9\n",
        "    \"art, beauty, life, purpose, philosophy, Lord Henry\"  # query_10\n",
        "]\n",
        "\n",
        "# Create notes for context\n",
        "notes = [\n",
        "    \"Should retrieve description of the portrait from early chapters\",\n",
        "    \"Character introduction - should be in first few chapters\",\n",
        "    \"Physical description of Dorian - early in book\",\n",
        "    \"Lord Henry's philosophy about beauty vs intellect\",\n",
        "    \"Basil's reason for not exhibiting - early conversation\",\n",
        "    \"Basil's story about meeting Dorian at Lady Brandon's party\",\n",
        "    \"Portrait's transformation - later in book\",\n",
        "    \"Lord Henry's famous speech about influence - Chapter 2\",\n",
        "    \"Central theme - portrait as mirror of soul\",\n",
        "    \"Lord Henry's aesthetic philosophy throughout the book\"\n",
        "]\n",
        "\n",
        "# Create dataframe\n",
        "qa_dev = pd.DataFrame({\n",
        "    'question': queries,\n",
        "    'acceptable_answer_keywords': keywords,\n",
        "    'notes': notes\n",
        "})\n",
        "\n",
        "# Save to CSV\n",
        "output_dir = Path(\"../data/interim\")\n",
        "output_dir.mkdir(parents=True, exist_ok=True)\n",
        "output_path = output_dir / \"qa_dev.csv\"\n",
        "qa_dev.to_csv(output_path, index=False)\n",
        "\n",
        "print(f\"âœ… Created QA development set with {len(queries)} questions\")\n",
        "print(f\"   Saved to: {output_path}\")\n",
        "print(f\"\\nðŸ“Š Query length distribution:\")\n",
        "for i, q in enumerate(queries, 1):\n",
        "    word_count = len(q.split())\n",
        "    print(f\"   Q{i}: {word_count} words - \\\"{q[:50]}{'...' if len(q) > 50 else ''}\\\"\")\n",
        "print(f\"\\nðŸ’¡ Note: Queries vary from {min(len(q.split()) for q in queries)} to {max(len(q.split()) for q in queries)} words - all work fine!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Evaluate Retrieval (Recall@k)\n",
        "\n",
        "For each question, retrieve top-k chunks and check if any contain the acceptable keywords.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Evaluate retrieval: for each Q, if any retrieved chunk contains a keyword â†’ hit.\n",
        "# Acceptance: print Recall@k summary.\n",
        "\n",
        "from src.retrieve import retrieve\n",
        "\n",
        "# Load QA set\n",
        "# For each question:\n",
        "#   - Retrieve top-k chunks\n",
        "#   - Check if any chunk contains acceptable keywords\n",
        "# Compute Recall@k\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Compose Answers with Quotes\n",
        "\n",
        "Test the answer composition pipeline with retrieved chunks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Compose an answer from retrieved chunks (no LLM), with quotes and citations.\n",
        "# Acceptance: dict with 'answer', 'quotes', 'used_chunks'.\n",
        "\n",
        "from src.compose import compose_answer\n",
        "\n",
        "# Test on a few example queries\n",
        "# Verify that answers include quotes and citations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Evaluate Groundedness\n",
        "\n",
        "Measure how well answers are grounded in quotes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Evaluate \"groundedness\": % of answers containing â‰¥1 quote AND each claim sentence overlaps tokens with at least one quote.\n",
        "# Hints:\n",
        "# 1) For a small QA set, run retrieve -> compose; check non-empty quotes.\n",
        "# 2) Token-overlap heuristic: for each answer sentence, require >= t shared content words with some quote (t ~ 2â€“3).\n",
        "# Acceptance:\n",
        "# - Print groundedness rate and sample diagnostics for 3 questions.\n",
        "\n",
        "# For each question:\n",
        "#   - Retrieve and compose answer\n",
        "#   - Check quote presence\n",
        "#   - Compute token overlap between answer sentences and quotes\n",
        "# Report groundedness and attribution scores\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Wire Gradio Demo\n",
        "\n",
        "Create a simple Gradio interface for interactive Q&A.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'src' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Force reload the app\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mimportlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m reload\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m reload(\u001b[43msrc\u001b[49m.app)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Launch the demo\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;66;03m# Test with a few questions interactively\u001b[39;00m\n\u001b[32m     14\u001b[39m demo = launch_app()\n",
            "\u001b[31mNameError\u001b[39m: name 'src' is not defined"
          ]
        }
      ],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Wire a simple Gradio demo using src/app.launch_app().\n",
        "\n",
        "\n",
        "\n",
        "# Force reload the app\n",
        "\n",
        "\n",
        "# Launch the demo\n",
        "# Test with a few questions interactively\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Prepare for Hugging Face Spaces Deployment\n",
        "\n",
        "Optional: Prepare the app for deployment to Hugging Face Spaces.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Prepare for Hugging Face Spaces deployment.\n",
        "# Hints:\n",
        "# 1) Ensure app entrypoint is src/app.py with a function `launch_app()` or `demo = gr.Interface(...)`.\n",
        "# 2) Create a `README.md` for the Space (use SPACE_CARD.md text).\n",
        "# 3) Runtime: set \"Hardware: CPU basic\", \"SDK: Gradio\", \"Space Timeout: 120s\".\n",
        "# Acceptance:\n",
        "# - Space builds successfully; interacts within ~2â€“5 seconds per query on CPU.\n",
        "\n",
        "# See SPACE_CARD.md for the README content to use in the Space.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "At this point, you should have:\n",
        "\n",
        "- âœ… QA development set created (`data/interim/qa_dev.csv`)\n",
        "- âœ… Recall@k evaluated (target: â‰¥ 0.8)\n",
        "- âœ… Groundedness evaluated (target: â‰¥ 0.95)\n",
        "- âœ… Answer composition tested with quotes and citations\n",
        "- âœ… Gradio demo working locally\n",
        "- âœ… (Optional) Space deployment ready\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "- Optional LLM rewrite step (keeps quotes, improves fluency)\n",
        "- Named-entity & character graph for richer answers\n",
        "- Multi-book corpus with per-source filtering\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Codecademy ML",
      "language": "python",
      "name": "codeacademy"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
