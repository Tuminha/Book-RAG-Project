{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Notebook 04: Evaluation & Gradio Demo\n",
        "\n",
        "## Goal\n",
        "\n",
        "Build a lightweight evaluation set, test answer composition with verbatim quotes, and wire up a Gradio demo.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Building a Tiny Gold QA Set\n",
        "\n",
        "Create 10–20 question-answer pairs manually:\n",
        "\n",
        "- **Questions**: Focused, answerable from the text (e.g., \"What does Lord Henry say about influence?\")\n",
        "- **Acceptable answer keywords**: Terms that should appear in retrieved chunks (e.g., \"influence\", \"young\", \"soul\")\n",
        "- **Notes**: Optional context about expected answer structure\n",
        "\n",
        "Save this as `data/interim/qa_dev.csv` with columns: `question`, `acceptable_answer_keywords`, `notes`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Metrics: Recall@k & Groundedness\n",
        "\n",
        "### Recall@k (Retrieval)\n",
        "\n",
        "For each question, check if at least one retrieved chunk (top-k) contains any of the acceptable answer keywords.\n",
        "\n",
        "- **Recall@5**: Proportion of questions where a gold-supporting chunk appears in top-5\n",
        "- **Target**: ≥ 0.8 (80% of questions have relevant chunks retrieved)\n",
        "\n",
        "### Groundedness (Composition)\n",
        "\n",
        "Measure how well answers are grounded in quotes:\n",
        "\n",
        "- **Quote presence**: % of answers with ≥1 quote\n",
        "- **Attribution score**: Mean fraction of answer sentences that share ≥2 content words with some quote\n",
        "- **Target**: Groundedness ≥ 0.95, Attribution ≥ 0.7\n",
        "\n",
        "### Latency (UX)\n",
        "\n",
        "Mean retrieval + compose time on CPU for one query. Target: < 2 seconds.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Answer Style Guide\n",
        "\n",
        "When composing answers:\n",
        "\n",
        "- **Length**: 2–4 sentences, ~100–140 words\n",
        "- **Tone**: Assertive but qualified (\"the text suggests…\", \"the narrator frames…\")\n",
        "- **No inventions**: Every factual clause must be traceable to a quote\n",
        "- **References**: Use [1], [2], [3] in the answer; match to citations list\n",
        "- **Quote selection**: Prefer one quote that defines, one that illustrates, and one that contrasts (when available)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Create QA Development Set\n",
        "\n",
        "Manually create a small QA dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Create a small QA dataframe: {question, acceptable_answer_keywords, notes}.\n",
        "# Acceptance: CSV written to data/interim/qa_dev.csv\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Create 10-20 QA pairs manually\n",
        "# Save to data/interim/qa_dev.csv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Evaluate Retrieval (Recall@k)\n",
        "\n",
        "For each question, retrieve top-k chunks and check if any contain the acceptable keywords.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Evaluate retrieval: for each Q, if any retrieved chunk contains a keyword → hit.\n",
        "# Acceptance: print Recall@k summary.\n",
        "\n",
        "from src.retrieve import retrieve\n",
        "\n",
        "# Load QA set\n",
        "# For each question:\n",
        "#   - Retrieve top-k chunks\n",
        "#   - Check if any chunk contains acceptable keywords\n",
        "# Compute Recall@k\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Compose Answers with Quotes\n",
        "\n",
        "Test the answer composition pipeline with retrieved chunks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Compose an answer from retrieved chunks (no LLM), with quotes and citations.\n",
        "# Acceptance: dict with 'answer', 'quotes', 'used_chunks'.\n",
        "\n",
        "from src.compose import compose_answer\n",
        "\n",
        "# Test on a few example queries\n",
        "# Verify that answers include quotes and citations\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Evaluate Groundedness\n",
        "\n",
        "Measure how well answers are grounded in quotes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Evaluate \"groundedness\": % of answers containing ≥1 quote AND each claim sentence overlaps tokens with at least one quote.\n",
        "# Hints:\n",
        "# 1) For a small QA set, run retrieve -> compose; check non-empty quotes.\n",
        "# 2) Token-overlap heuristic: for each answer sentence, require >= t shared content words with some quote (t ~ 2–3).\n",
        "# Acceptance:\n",
        "# - Print groundedness rate and sample diagnostics for 3 questions.\n",
        "\n",
        "# For each question:\n",
        "#   - Retrieve and compose answer\n",
        "#   - Check quote presence\n",
        "#   - Compute token overlap between answer sentences and quotes\n",
        "# Report groundedness and attribution scores\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Wire Gradio Demo\n",
        "\n",
        "Create a simple Gradio interface for interactive Q&A.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Wire a simple Gradio demo using src/app.launch_app().\n",
        "\n",
        "from src.app import launch_app\n",
        "\n",
        "# Launch the demo\n",
        "# Test with a few questions interactively\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Prepare for Hugging Face Spaces Deployment\n",
        "\n",
        "Optional: Prepare the app for deployment to Hugging Face Spaces.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# === TODO (you code this) ===\n",
        "# Prepare for Hugging Face Spaces deployment.\n",
        "# Hints:\n",
        "# 1) Ensure app entrypoint is src/app.py with a function `launch_app()` or `demo = gr.Interface(...)`.\n",
        "# 2) Create a `README.md` for the Space (use SPACE_CARD.md text).\n",
        "# 3) Runtime: set \"Hardware: CPU basic\", \"SDK: Gradio\", \"Space Timeout: 120s\".\n",
        "# Acceptance:\n",
        "# - Space builds successfully; interacts within ~2–5 seconds per query on CPU.\n",
        "\n",
        "# See SPACE_CARD.md for the README content to use in the Space.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "At this point, you should have:\n",
        "\n",
        "- ✅ QA development set created (`data/interim/qa_dev.csv`)\n",
        "- ✅ Recall@k evaluated (target: ≥ 0.8)\n",
        "- ✅ Groundedness evaluated (target: ≥ 0.95)\n",
        "- ✅ Answer composition tested with quotes and citations\n",
        "- ✅ Gradio demo working locally\n",
        "- ✅ (Optional) Space deployment ready\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "- Optional LLM rewrite step (keeps quotes, improves fluency)\n",
        "- Named-entity & character graph for richer answers\n",
        "- Multi-book corpus with per-source filtering\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
